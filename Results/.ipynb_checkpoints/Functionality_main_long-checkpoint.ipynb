{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.font_manager\n",
    "from matplotlib import style\n",
    "style.use('seaborn') or plt.style.use('seaborn')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "import scipy.stats as ss\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Lasso, MultiTaskLasso, Ridge, ElasticNet\n",
    "import math\n",
    "\n",
    "#from regressors import stats\n",
    "\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import colorsys\n",
    "import matplotlib.colors as cconv\n",
    "from statannot import add_stat_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_bib as mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(var, X_cat_, color_dict_):\n",
    "\n",
    "    X_factors = list(X_cat_.factors)\n",
    "    X_name = list(X_cat_.newname)\n",
    "\n",
    "    index_code = X_name.index(var)\n",
    "    cat = X_factors[index_code]\n",
    "\n",
    "    return color_dict_[cat]\n",
    "\n",
    "color_dict = {}\n",
    "color_dict['Country'] = [40/255, 43/255, 95/255]\n",
    "color_dict['HF'] = '#A31300'\n",
    "color_dict['LSF'] = '#FF9E0D'\n",
    "color_dict['PSF'] = '#FF4800'\n",
    "color_dict['SDHF'] = '#1C4F9E'\n",
    "color_dict['DF'] = '#009E32'\n",
    "\n",
    "def get_bar_colors(data_, X_cat_):\n",
    "    color_dict = {}\n",
    "    color_dict['Country'] = [40/255, 43/255, 95/255]\n",
    "    color_dict['HF'] = '#A31300'\n",
    "    color_dict['LSF'] = '#FF9E0D'\n",
    "    color_dict['PSF'] = '#FF4800'\n",
    "    color_dict['SDHF'] = '#1C4F9E'\n",
    "    color_dict['DF'] = '#009E32'\n",
    "    \n",
    "    #color_dict = {}\n",
    "    #color_dict = {'DF':'#5975a4', 'MF':'#cc8963', 'SF':'#5f9e6e', 'SLF':'#b55d60',\n",
    "    #              'n':'#857aab', 'country':'#8d7866'}#, '#d095bf'}\n",
    "\n",
    "    X_factors = list(X_cat_.factors)\n",
    "    X_name = list(X_cat_.newname)\n",
    "\n",
    "    bar_color_total = []\n",
    "    for i in list(data_.Features):\n",
    "        if(i == 'Country'):\n",
    "            bar_color_total.append(color_dict['Country'])\n",
    "            continue\n",
    "        index_code = X_name.index(i)\n",
    "        cat = X_factors[index_code]\n",
    "        bar_color_total.append(color_dict[cat])\n",
    "    return bar_color_total\n",
    "\n",
    "\n",
    "def plot_estimate_value(regression_model, X_cat_ = [], title = '',  xlim =[0, 2] ,fig_size = (8,12), size = 14, pvalue_type = 'False'):\n",
    "\n",
    "    df = regression_model[0]\n",
    "    df.index.name = 'Features'\n",
    "    df = df.iloc[1:-3, 0:-1]\n",
    "\n",
    "    for i in range(3):\n",
    "            #print(i, X_RAW_edu_level[X_RAW_edu_level.columns[i]].dtype)\n",
    "        df[df.columns[i]] = np.abs(pd.to_numeric(df[df.columns[i]],errors = 'coerce'))\n",
    "\n",
    "    df = df.reset_index()\n",
    "    data = df.sort_values('Estimate', ascending=False)\n",
    "\n",
    "    \n",
    "    if(len(X_cat_)>0):\n",
    "        \n",
    "        bar_color = get_bar_colors(data, X_cat_)\n",
    "        \n",
    "        plt.title(title)\n",
    "        sns.barplot(x=\"Estimate\", y=\"Features\", data = data, palette =bar_color)\n",
    "        plt.xlim(xlim)\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        sns.barplot(x=\"Estimate\", y=\"Features\", data = data, color = 'darkblue')\n",
    "        plt.xlim(xlim)\n",
    "\n",
    "\n",
    "    y_step=0  \n",
    "    for i in range(df.shape[0]):\n",
    "        if(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.01):\n",
    "            color = 'green'\n",
    "        elif(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.05):\n",
    "            color = 'gray'\n",
    "        else:\n",
    "            color = 'red'        \n",
    "        \n",
    "        if(pvalue_type == 'color'):\n",
    "                plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]-0.005, y_step, \n",
    "                                 '' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],2)),\n",
    "                                 size= size, rotation=0.,\n",
    "                                 ha=\"left\", va=\"center\", color = 'white',\n",
    "                                 bbox=dict(boxstyle=\"round\",\n",
    "                                           ec=color,\n",
    "                                            fc=color,\n",
    "                                           )\n",
    "                                 )\n",
    "                y_step+=1\n",
    "\n",
    "        elif(pvalue_type == 'value'):\n",
    "                plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                 '(' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],8))+')',\n",
    "                                 size= size, rotation=0.,\n",
    "                                 ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                 )\n",
    "                y_step+=1\n",
    "        else:\n",
    "            if(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]<0.01):\n",
    "                \n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '**',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )\n",
    "            elif(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]>= 0.01 and df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]<0.05):\n",
    "                  \n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '*',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )  \n",
    "            else:\n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )      \n",
    "            y_step+=1\n",
    "            \n",
    "         \n",
    "    text_diff =xlim[1]/2.2\n",
    "    plt.text(xlim[1] - text_diff, df.shape[0]-1.5,r'$ R^2 $(' + str(np.round(regression_model[1],2)) + ') \\t$F^2 $(' + str(np.round(regression_model[3],2)) + ')',\n",
    "                             size= 12, rotation=0.,\n",
    "                             ha=\"left\", va=\"center\", color = 'black',\n",
    "                             bbox=dict(boxstyle=\"round\",\n",
    "                                       ec='gray',\n",
    "                                        fc='gray',\n",
    "                                       )\n",
    "                             )\n",
    "\n",
    "\n",
    "    plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_estimate_value_no_sort(regression_model, title = '',  xlim =[0, 2] ,fig_size = (8,12), size = 14, ylabel = True, ylabelR = False ):\n",
    "\n",
    "    df = regression_model\n",
    "    df.index.name = 'Features'\n",
    "    df = df.iloc[1:-3, 0:-1]\n",
    "\n",
    "    for i in range(3):\n",
    "            #print(i, X_RAW_edu_level[X_RAW_edu_level.columns[i]].dtype)\n",
    "        df[df.columns[i]] = np.abs(pd.to_numeric(df[df.columns[i]],errors = 'coerce'))\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    #plt.figure(figsize=(8,12))\n",
    "    plt.title(title)\n",
    "    sns.barplot(x=\"Estimate\", y=\"Features\", data = df, color = 'Brown')\n",
    "    if(ylabel == False):\n",
    "        plt.ylabel('')\n",
    "        plt.yticks([])\n",
    "    plt.xlim(xlim)\n",
    "    \n",
    "    if(ylabelR):\n",
    "        plt.tick_params (axis = 'y', which = 'both', labelleft = False, labelright = True)\n",
    "\n",
    "    y_step=0  \n",
    "    for i in range(df.shape[0]):\n",
    "        if(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.01):\n",
    "            color = 'green'\n",
    "        elif(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.05):\n",
    "            color = 'gray'\n",
    "        else:\n",
    "            color = 'red'        \n",
    "        \n",
    "        plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]-0.005, y_step, \n",
    "                         '' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)),\n",
    "                         size= size, rotation=0.,\n",
    "                         ha=\"left\", va=\"center\", color = 'white',\n",
    "                         bbox=dict(boxstyle=\"round\",\n",
    "                                   ec=color,\n",
    "                                    fc=color,\n",
    "                                   )\n",
    "                         )\n",
    "        y_step+=1\n",
    "        \n",
    "\n",
    "        \n",
    "def conf_interval(database):\n",
    "    r_squared_list = []\n",
    "\n",
    "    y_database = database['Barthel']\n",
    "    X_database = database.drop('Barthel', axis=1)\n",
    "\n",
    "    for i in range(0,100,10):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "        opt_Ridge = BayesSearchCV(\n",
    "            Ridge(),\n",
    "            {\n",
    "                'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                'max_iter': (1000, 10000, 100000, 1000000),\n",
    "            },\n",
    "            n_iter=10,\n",
    "            random_state=i, \n",
    "            scoring='r2',\n",
    "            cv=3\n",
    "        )\n",
    "\n",
    "        opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "        r_squared_list.append(opt_Ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    print(r'99% confidence interval (+-', np.round(np.std(r_squared_list)*2.58, 4),')')\n",
    "    return np.round(np.std(r_squared_list)*2.58, 4)\n",
    "\n",
    "\n",
    "def adj_r2_score_and_r2_score(clf, X, y):\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    p = X.shape[1]  # Number of features\n",
    "    r_squared = r2_score(y, clf.predict(X))\n",
    "    return [1 - (1 - r_squared) * ((n - 1) / (n - p - 1)), r_squared]\n",
    "\n",
    "\n",
    "def mse(clf, X, y):\n",
    "    return mean_squared_error(y, clf.predict(X))\n",
    "\n",
    "def rmse(clf, X, y):\n",
    "    mse = mean_squared_error(y, clf.predict(X))\n",
    "    return math.sqrt(mse)    \n",
    "\n",
    "def coef_se(clf, X, y):\n",
    "    n = X.shape[0]\n",
    "    X1 = np.hstack((np.ones((n, 1)), np.matrix(X)))\n",
    "    se_matrix = scipy.linalg.sqrtm(\n",
    "        metrics.mean_squared_error(y, clf.predict(X)) *\n",
    "        np.linalg.inv(X1.T * X1)\n",
    "    )\n",
    "    return np.diagonal(se_matrix)\n",
    "\n",
    "def coef_tval(clf, X, y):\n",
    "    a = np.array(clf.intercept_ / coef_se(clf, X, y)[0])\n",
    "    b = np.array(clf.coef_ / coef_se(clf, X, y)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "def coef_tval_XGB_tree(clf, X, y):\n",
    "    a = np.nan\n",
    "    b = np.array(clf.feature_importances_ / coef_se(clf, X, y)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "def coef_pval(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    t = coef_tval(clf, X, y)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "def coef_pval_XGB_tree(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    t = coef_tval_XGB_tree(clf, X, y)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "def residuals(clf, X, y, r_type='standardized'):\n",
    "\n",
    "    # Make sure value of parameter 'r_type' is one we recognize\n",
    "    assert r_type in ('raw', 'standardized', 'studentized'), (\n",
    "        \"Invalid option for 'r_type': {0}\".format(r_type))\n",
    "    y_true = y.view(dtype='float')\n",
    "    # Use classifier to make predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    # Make sure dimensions agree (Numpy still allows subtraction if they don't)\n",
    "    assert y_true.shape == y_pred.shape, (\n",
    "        \"Dimensions of y_true {0} do not match y_pred {1}\".format(y_true.shape,\n",
    "                                                                  y_pred.shape))\n",
    "    # Get raw residuals, or standardized or standardized residuals\n",
    "    resids = y_pred - y_true\n",
    "    if r_type == 'standardized':\n",
    "        resids = resids / np.std(resids)\n",
    "    elif r_type == 'studentized':\n",
    "        # Prepare a blank array to hold studentized residuals\n",
    "        studentized_resids = np.zeros(y_true.shape[0], dtype='float')\n",
    "        # Calcluate hat matrix of X values so you can get leverage scores\n",
    "        hat_matrix = np.dot(\n",
    "            np.dot(X, np.linalg.inv(np.dot(np.transpose(X), X))),\n",
    "            np.transpose(X))\n",
    "        # For each point, calculate studentized residuals w/ leave-one-out MSE\n",
    "        for i in range(y_true.shape[0]):\n",
    "            # Make a mask so you can calculate leave-one-out MSE\n",
    "            mask = np.ones(y_true.shape[0], dtype='bool')\n",
    "            mask[i] = 0\n",
    "            loo_mse = np.average(resids[mask] ** 2, axis=0)  # Leave-one-out MSE\n",
    "            # Calculate studentized residuals\n",
    "            studentized_resids[i] = resids[i] / np.sqrt(\n",
    "                loo_mse * (1 - hat_matrix[i, i]))\n",
    "        resids = studentized_resids\n",
    "    return resids\n",
    "\n",
    "\n",
    "def f_squared(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    return r_squared  / (1 - r_squared)\n",
    "\n",
    "def f_stat(clf, X, y):\n",
    "    \"\"\"Calculate summary F-statistic for beta coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : sklearn.linear_model\n",
    "        A scikit-learn linear model classifier with a `predict()` method.\n",
    "    X : numpy.ndarray\n",
    "        Training data used to fit the classifier.\n",
    "    y : numpy.ndarray\n",
    "        Target training values, of shape = [n_samples].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F-statistic value.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    return (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "\n",
    "def f_stat_pvalue(clf, X, y):\n",
    "    \"\"\"Calculate summary F-statistic p value for beta coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : sklearn.linear_model\n",
    "        A scikit-learn linear model classifier with a `predict()` method.\n",
    "    X : numpy.ndarray\n",
    "        Training data used to fit the classifier.\n",
    "    y : numpy.ndarray\n",
    "        Target training values, of shape = [n_samples].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F-statistic p value.\n",
    "    \"\"\"\n",
    "    n = X.shape[0] # Esto se extrae par los grados de libertad del numeador y el denomindor (no. predictores, no. sujetos - no. predictores-1)\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    \n",
    "    return np.round(scipy.stats.f.sf(f_stat(clf, X, y), n, (n - p - 1)), 15)\n",
    "\n",
    "def compute_f_statistics(clf, X, y):\n",
    "    return [f_stat(clf, X, y), f_stat_pvalue(clf, X, y)]\n",
    "\n",
    "\n",
    "\n",
    "def summary(clf, X, y, xlabels=None, regressor = ''):\n",
    "\n",
    "    print('Resumen del regresor ' + regressor + '\\n')\n",
    "    \n",
    "    ncols = X.shape[1]\n",
    "    if xlabels is None:\n",
    "        xlabels = np.array(\n",
    "            ['x{0}'.format(i) for i in range(1, ncols + 1)], dtype='str')\n",
    "    elif isinstance(xlabels, (tuple, list)):\n",
    "        xlabels = np.array(xlabels, dtype='str')\n",
    "\n",
    "    # Create data frame of coefficient estimates and associated stats\n",
    "    coef_df = pd.DataFrame(\n",
    "        index=['_intercept'] + list(xlabels),\n",
    "        columns=['Estimate','t value', 'p value']\n",
    "    )\n",
    "    \n",
    "    if(regressor == 'XGBRegressor'):\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "            (np.round(np.array([clf.intercept_[0]]), 6), np.round((clf.coef_), 6)))\n",
    "        #coef_df['MSE'] = np.round(mse(clf, X, y), 6)\n",
    "        #coef_df['RMSE'] = np.round(rmse(clf, X, y), 6)\n",
    "        coef_df['t value'] = np.round(coef_tval(clf, X, y), 4)\n",
    "        coef_df['p value'] = np.round(coef_pval(clf, X, y), 20)\n",
    "        # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "            'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "            '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "            'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "            '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "            'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "        # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "        \n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "           r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "            f_sq))\n",
    "    elif(regressor == 'XGBRegressorNoLinear'):\n",
    "        coef_df = pd.DataFrame(\n",
    "            index=['_intercept'] + list(xlabels),\n",
    "            columns=['Estimate','t value', 'p value']\n",
    "        )\n",
    "\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "                (np.round(np.array([np.nan]), 6), np.round((clf.feature_importances_), 6)))\n",
    "\n",
    "        coef_df['t value'] = np.round(coef_tval_XGB_tree(clf, X, y), 4)\n",
    "        coef_df['p value'] = np.round(coef_pval_XGB_tree(clf, X, y), 20)\n",
    "            # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "                'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "                '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "                'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "                '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "                'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "            # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "\n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "               r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "                f_sq))\n",
    "    else:\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "            (np.round(np.array([clf.intercept_]), 6), np.round((clf.coef_), 6)))\n",
    "        #coef_df['MSE'] = np.round(mse(clf, X, y), 6)\n",
    "        #coef_df['RMSE'] = np.round(rmse(clf, X, y), 6)\n",
    "        coef_df['t value'] = np.round(coef_tval(clf, X, y), 4)\n",
    "        coef_df['p value'] = np.round(coef_pval(clf, X, y), 20)\n",
    "        # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "            'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "            '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "            'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "            '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "            'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "        # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        \n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "        \n",
    "        mse_ = np.round(mse(clf, X, y), 6)\n",
    "        rmse_ = np.round(rmse(clf, X, y), 6)\n",
    "        \n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "           r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "            f_sq))\n",
    "        print('MSE:  {0:.5f}'.format(\n",
    "            mse_))        \n",
    "        print('RMSE:  {0:.5f}'.format(\n",
    "            rmse_))        \n",
    "    print('---------------------------------------------------------------------------\\n\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    empty_str = []\n",
    "    for i in range(coef_df.shape[0]):\n",
    "        empty_str.append('')\n",
    "    \n",
    "    coef_df['value'] = empty_str\n",
    "    \n",
    "    coef_df = coef_df.T\n",
    "    coef_df['R-squared'] = ['','','', r_sq]\n",
    "    coef_df['Adjusted R-squared'] = ['','','', r_sq_adj]\n",
    "    coef_df['F-squared'] = ['','','', f_sq]\n",
    "    #coef_df['MSE'] = ['','','', mse_]\n",
    "    #coef_df['RMSE'] = ['','','', rmse_]\n",
    "    return [coef_df.T, r_sq, r_sq_adj, f_sq, mse_, rmse_]\n",
    "\n",
    "\n",
    "\n",
    "def summary_validation(X_, y_, opt_Ridge_ = ''):\n",
    "    \n",
    "    from statsmodels.stats.multitest import fdrcorrection\n",
    "    #from statsmodels.stats.multitest import fisher_combined_pvalues\n",
    "    from scipy.stats import combine_pvalues\n",
    "\n",
    "\n",
    "\n",
    "    lista_vars = list(X_)\n",
    "    \n",
    "    coef_array = np.zeros([len(lista_vars)+1, 10])\n",
    "    coef_t_value = np.zeros([len(lista_vars)+1, 10])\n",
    "    coef_p_value = np.zeros([len(lista_vars)+1, 10])\n",
    "\n",
    "    r2_list =[]\n",
    "    f2_list =[]\n",
    "    f_list =[]\n",
    "    f_p_value_list =[]\n",
    "    mse_list =[]\n",
    "    rmse_list =[]\n",
    "    for iter_ in tqdm(range(0,10), total=len(range(0, 10)),leave=True, mininterval=1, ascii=True,\n",
    "                            colour='green', desc='Training model'):\n",
    "        # Stratified split\n",
    "        \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.30, random_state=iter_)\n",
    "\n",
    "        model = Ridge(alpha=0.1, solver = opt_Ridge_.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge_.best_params_['max_iter'])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        coef_array[0,iter_] = np.abs(model.intercept_)\n",
    "        coef_array[1::,iter_] = np.abs(model.coef_)\n",
    "\n",
    "        #print(iter_, coef_se(model, X_test, y_test), np.abs(coef_se(model, X_test, y_test)))\n",
    "\n",
    "        coef_t_value[:,iter_] = np.abs(np.round(coef_tval(model, X_test, y_test), 30))\n",
    "        coef_p_value[:,iter_] = np.round(coef_pval(model, X_test, y_test), 30)\n",
    "\n",
    "        #print(coef_t_value[:,iter_], coef_p_value[:,iter_])\n",
    "        #coef_df['t value'] = np.abs(np.round(coef_tval(clf, X, y), 4))\n",
    "        #coef_df['p value'] = np.round(coef_pval(clf, X, y), 20)\n",
    "\n",
    "        f2_list.append(f_squared(model, X_test, y_test))\n",
    "        f_list.append(f_stat(model, X_test, y_test))\n",
    "        f_p_value_list.append(f_stat_pvalue(model, X_test, y_test))\n",
    "        r2_list.append(r2_score(y_test, model.predict(X_test)))\n",
    "        \n",
    "        mse_list.append(np.round(mse(model, X_test, y_test), 6))\n",
    "        rmse_list.append(np.round(rmse(model, X_test, y_test), 6))\n",
    "\n",
    "\n",
    "    r2_list_array = np.array(r2_list)\n",
    "    # finding the 1st quartile\n",
    "    q1 = np.quantile(r2_list_array, 0.25)\n",
    " \n",
    "    # finding the 3rd quartile\n",
    "    q3 = np.quantile(r2_list_array, 0.75)\n",
    "    med = np.median(r2_list_array)\n",
    "\n",
    "    # finding the iqr region\n",
    "    iqr = q3-q1\n",
    "\n",
    "    # finding upper and lower whiskers\n",
    "    upper_bound = q3+(1.5*iqr)\n",
    "    lower_bound = q1-(1.5*iqr)\n",
    "    #print(iqr, upper_bound, lower_bound)\n",
    "\n",
    "    index_del = []     \n",
    "\n",
    "    for i in range(r2_list_array.shape[0]):\n",
    "        if((r2_list_array[i]<= lower_bound) | (r2_list_array[i] >= upper_bound)):\n",
    "            index_del.append(i)\n",
    "\n",
    "    \n",
    "    \n",
    "    r2_list = list(np.delete(r2_list_array,index_del, axis=0 ))\n",
    "\n",
    "    coef_df = pd.DataFrame(\n",
    "            index=['_intercept'] + lista_vars,\n",
    "            columns=['Estimate mean','t value mean', 'p value mean', 'p value fdr', \n",
    "                     'p value stouffer', 'p value fisher', \n",
    "                      'p value tippett', 'Estimate std']\n",
    "        )\n",
    "    \n",
    "    df_coef_value = {}\n",
    "    df_p_value = {}\n",
    "    df_t_value = {}\n",
    "    lista_vars_interc = ['_intercept'] + lista_vars\n",
    "    for i in range(len(lista_vars_interc)):\n",
    "        df_coef_value[lista_vars_interc[i]] = coef_array[i,:]\n",
    "        df_p_value[lista_vars_interc[i]] = coef_p_value[i,:]\n",
    "        df_t_value[lista_vars_interc[i]] = coef_t_value[i,:]\n",
    "\n",
    "    coef_array  = np.delete(coef_array,index_del, axis=1 )\n",
    "    coef_t_value = np.delete(coef_t_value,index_del, axis=1 )\n",
    "    coef_p_value = np.delete(coef_p_value,index_del, axis=1 )\n",
    "    \n",
    "    \n",
    "    coef_array_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_t_value_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_mean = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_fdr = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_stouffer = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_fisher = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_pearson = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_tippett = np.zeros([len(lista_vars)+1, 1])\n",
    "    coef_p_value_mudholkar_george = np.zeros([len(lista_vars)+1, 1])\n",
    "\n",
    "    coef_array_std = np.zeros([len(lista_vars)+1, 1])\n",
    "\n",
    "    print('shape',coef_array.shape, coef_t_value.shape, coef_p_value.shape, len(index_del))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #coef_p_value = coef_p_value + 1e-9\n",
    "    coef_p_value = np.where(coef_p_value == 0, 1e-30, coef_p_value)\n",
    "    \n",
    "    f_p_value_list_a = np.array(f_p_value_list)\n",
    "    f_p_value_list_a = np.where(f_p_value_list_a == 0, 1e-30, f_p_value_list_a)\n",
    "    f_p_value_list = list(f_p_value_list_a)\n",
    "    \n",
    "    for i in range(len(lista_vars)+1):\n",
    "        coef_array_mean[i] = coef_array[i,:].mean()\n",
    "        coef_t_value_mean[i]= coef_t_value[i,:].mean()\n",
    "        coef_p_value_mean[i]= coef_p_value[i,:].mean()\n",
    "        coef_p_value_fdr[i]= fdrcorrection(coef_p_value[i,:], alpha=0.05, is_sorted=False )[1].max()\n",
    "        coef_p_value_stouffer[i] = combine_pvalues(coef_p_value[i,:], method='stouffer')[1]\n",
    "        coef_p_value_fisher[i] = combine_pvalues(coef_p_value[i,:], method='fisher')[1]\n",
    "        #coef_p_value_pearson[i] = combine_pvalues(coef_p_value[i,:], method='pearson')[1]\n",
    "        coef_p_value_tippett[i] = combine_pvalues(coef_p_value[i,:], method='tippett')[1]\n",
    "        #coef_p_value_mudholkar_george[i] = combine_pvalues(coef_p_value[i,:], method='mudholkar_george')[1]\n",
    "        \n",
    "        coef_array_std[i] = coef_array[i,:].std()\n",
    "        # Se corrige por FDR y se reporta el valor máximo\n",
    "\n",
    "    coef_df['Estimate mean'] = coef_array_mean\n",
    "    coef_df['t value mean'] = coef_t_value_mean\n",
    "    coef_df['p value mean'] = coef_p_value_mean\n",
    "    coef_df['p value fdr'] = coef_p_value_fdr\n",
    "    coef_df['p value stouffer'] = coef_p_value_stouffer\n",
    "    coef_df['p value fisher'] = coef_p_value_fisher\n",
    "    #coef_df['p value pearson'] = coef_p_value_pearson\n",
    "    coef_df['p value tippett'] = coef_p_value_tippett\n",
    "    #coef_df['p value mudholkar_george'] = coef_p_value_mudholkar_george\n",
    "    coef_df['Estimate std'] = coef_array_std\n",
    "\n",
    "\n",
    "    empty_str = []\n",
    "    for i in range(coef_df.shape[0]):\n",
    "        empty_str.append('')\n",
    "\n",
    "    coef_df['value'] = empty_str\n",
    "\n",
    "    coef_df = coef_df.T\n",
    "    coef_df['R-squared'] = ['','','','','','','', '', np.mean(r2_list)]\n",
    "    coef_df['CI'] = ['','','','','','','', '', np.std(r2_list)*1.96]\n",
    "    coef_df['F-squared'] = ['','','','','','','', '', np.mean(f2_list)]\n",
    "    coef_df['F'] = ['','','','','','','', '', np.mean(f_list)]\n",
    "    coef_df['F-pvalue fdr'] = ['','','','','','','', '', fdrcorrection(f_p_value_list, alpha=0.05, is_sorted=False )[1].max()]\n",
    "    coef_df['F-pvalue stouffer'] = ['','','','','','','', '', combine_pvalues(f_p_value_list, method='stouffer')[1]]  \n",
    "    coef_df['F-pvalue fisher'] = ['','','','','','','', '', combine_pvalues(f_p_value_list, method='fisher')[1]]  \n",
    "    \n",
    "    #coef_df['F-pvalue pearson'] = ['','','','','','','','','', '', combine_pvalues(f_p_value_list, method='pearson')[1]]  \n",
    "    coef_df['F-pvalue tippett'] = ['','','','','','','', '',combine_pvalues(f_p_value_list, method='tippett')[1]]  \n",
    "    #coef_df['F-pvalue mudholkar_george'] = ['','','','','','','','','', '', combine_pvalues(f_p_value_list, method='mudholkar_george')[1]]  \n",
    "    #coef_df['F-pvalue_fdr'] = ['','','', fdrcorrection(f_p_value_list, alpha=0.05, method='indep', is_sorted=False )]\n",
    "    coef_df = coef_df.T\n",
    "\n",
    "    print('Coefficients:')\n",
    "    display(coef_df)\n",
    "    print(10*'---')\n",
    "        \n",
    "    #print(np.mean(r2_list),  np.std(r2_list)*1.96)\n",
    "    \n",
    "    return [coef_df, df_coef_value, df_p_value, df_t_value, mse_list, rmse_list, r2_list, f2_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    sig = 1 / (1 + math.exp(-x))\n",
    "    return sig\n",
    "\n",
    "def get_order_var(model_):\n",
    "\n",
    "    df = model_[0]['Estimate'].iloc[1:-3]\n",
    "    df = np.abs(pd.to_numeric(df,errors = 'coerce'))\n",
    "    df = list(df.sort_values(ascending=False).index)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "def colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n",
    "    c1=np.array(mpl.colors.to_rgb(c1))\n",
    "    c2=np.array(mpl.colors.to_rgb(c2))\n",
    "    return mpl.colors.to_hex((1-mix)*c1 + mix*c2)\n",
    "\n",
    "def plot_longitudinal(var, y_countrys_, models_, market_size_type = 'log',xlim = [0.25, 5.75], \n",
    "                      years = [2006, 2014, 2016],ylim = [10.0, 11.12], size_init = 1, size_mult = 40, \n",
    "                      size_edge_market = 2, xlabel = True, ylabel = True, ylabel_text = 'MMSE',color_dict_ = '', X_cat_ = ''):\n",
    "    \n",
    "    #X_cat = pd.read_csv('Data/var_name_color_code.csv', encoding='latin-1', sep=\";\")\n",
    "    \n",
    "    color = get_color(var, X_cat_, color_dict_)\n",
    "    edge_color = 'black'\n",
    "\n",
    "    y = []\n",
    "    for i in range(len(y_countrys_)):\n",
    "        y.append(y_countrys_[i].mean())\n",
    "        #y = [y_countrys_[0].mean(), y_countrys_[1].mean(),\n",
    "        #     y_countrys_[2].mean(), y_countrys_[3].mean(),\n",
    "        #     y_countrys_[4].mean()]\n",
    "\n",
    "\n",
    "    #var_ind = list(X_cat.oldname).index(var)\n",
    "    #var_ = list(X_cat.newname)[var_ind]\n",
    "    \n",
    "\n",
    "     \n",
    "    plt.plot(range(1, len(y_countrys_)+1), y, 'k--')\n",
    "\n",
    "    for i in range(len(models_)):\n",
    "        df = models_[i][0]\n",
    "        y = np.abs(df.iloc[df.index.get_loc(var), df.columns.get_loc('Estimate')])\n",
    "        \n",
    "        if(market_size_type=='log'):\n",
    "            if(y < 0.003):\n",
    "                markersize_ = 0\n",
    "            else:\n",
    "                markersize_ = size_mult*(5 + np.log(y))\n",
    "        elif(market_size_type=='log1'):\n",
    "            markersize_ = np.log(100000*y)\n",
    "        elif(market_size_type=='sigmoide'):\n",
    "            markersize_ = size_mult*sigmoid(y)\n",
    "        else:\n",
    "            markersize_ = np.abs(size_mult*y) \n",
    "        \n",
    "        markersize_ +=size_init\n",
    "        plt.plot(i + 1, y_countrys_[i].mean(), marker=\"o\",  \n",
    "                 markeredgewidth = size_edge_market, markeredgecolor = edge_color, markerfacecolor = color, markersize= markersize_)\n",
    "        \n",
    "        \n",
    "        p = df['p value'][var]\n",
    "        \n",
    "        p_text = ''\n",
    "        if(p<= 0.01):\n",
    "            p_text = '**'\n",
    "        elif((p<= 0.05)):\n",
    "            p_text = '*'\n",
    "        \n",
    "        \n",
    "        plt.text(i + 1 , y_countrys_[i].mean(), p_text,\n",
    "                                             size= 22, rotation=0.,\n",
    "                                             ha=\"center\", va=\"top\", color = 'black',\n",
    "                                            # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                            #           ec='gray',\n",
    "                                            #           fc='gray',\n",
    "                                            #          )\n",
    "                                             );\n",
    "        \n",
    "        #plt.text(i + 1 , ylim[0] - 0.25, str(round(markersize_,2)),\n",
    "         #                                    size= 8, rotation=0.,\n",
    "          #                                   ha=\"center\", va=\"top\", color = 'black',\n",
    "                                            # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                            #           ec='gray',\n",
    "                                            #           fc='gray',\n",
    "                                            #          )\n",
    "           #                                  );\n",
    "       # print('')\n",
    "        r2 = str(np.round(models_[i][1],2))\n",
    "        f2 = str(np.round(models_[i][3],2))\n",
    "        #plt.text(i+1,10.15,r'$R^2$ ('+r2+') \\n$F^2$ ('+f2+')',\n",
    "        #                         size= 12, rotation=0.,\n",
    "        #                         ha=\"center\", va=\"top\", color = 'black',\n",
    "        #                         bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "        #                                   ec='ghostwhite',\n",
    "        #                                    fc='ghostwhite',\n",
    "        #                                  )\n",
    "        #                         );\n",
    "\n",
    "    plt.text(xlim[1] , ylim[1], var,\n",
    "                                         size= 13, rotation=0.,\n",
    "                                         ha=\"right\", va=\"center\", color = 'black',\n",
    "                                        # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                        #           ec='gray',\n",
    "                                        #           fc='gray',\n",
    "                                        #          )\n",
    "                                         );\n",
    "\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim([ylim[0]-0.25, ylim[1]+0.25])\n",
    "    \n",
    "\n",
    "    if(xlabel):\n",
    "        plt.xticks(list(range(1, len(years)+1)), years)\n",
    "    else:\n",
    "        plt.xticks(list(range(1, len(years)+1)), ['', '', '', '', ''])\n",
    "    \n",
    "    if(ylabel):\n",
    "        plt.yticks(np.round(np.linspace(ylim[0], ylim[1], 4),2), np.round(np.linspace(ylim[0], ylim[1], 4),1))\n",
    "    else:\n",
    "        plt.yticks(np.round(np.linspace(ylim[0], ylim[1], 4),2), ['', '', '', ''])\n",
    "\n",
    "    if(ylabel):\n",
    "        plt.ylabel(ylabel_text, fontsize=13)\n",
    "    else:\n",
    "        plt.ylabel('')\n",
    "        \n",
    "    if(xlabel):\n",
    "        plt.xlabel('Years', fontsize=13)\n",
    "    else:\n",
    "        plt.xlabel('')\n",
    "        \n",
    "def plot_r2_f2(models_, xlim = [0.25, 5.75], ylim = [10.0, 11.12]):\n",
    "    \n",
    "    X_cat = pd.read_csv('Data/var_name_color_code.csv', encoding='latin-1', sep=\";\")\n",
    "    \n",
    "    \n",
    "    plt.plot(0.0)\n",
    "\n",
    "    for i in range(len(models)):\n",
    "\n",
    "        r2 = str(np.round(models_[i][1],2))\n",
    "        f2 = str(np.round(models_[i][3],2))\n",
    "        plt.text(i+1,1,r'$R^2$ ('+r2+') \\n$F^2$ ('+f2+')',\n",
    "                                 size= 12, rotation=0.,\n",
    "                                 ha=\"center\", va=\"top\", color = 'black',\n",
    "                                 #bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                 #          ec='ghostwhite',\n",
    "                                 #           fc='ghostwhite',\n",
    "                                 #         )\n",
    "                                 );\n",
    "        \n",
    "    \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim([ylim[0], ylim[1]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_chile = pd.read_excel('../Data/cross/SABE_chile.xlsx') # to use it for columns order\n",
    "sabe_chile = sabe_chile.iloc[:,1::]\n",
    "\n",
    "sabe_costarica = pd.read_excel('../Data/long/SABE_costarica_long.xlsx')\n",
    "sabe_costarica = sabe_costarica.iloc[:,1::]\n",
    "\n",
    "sabe_china_2011_2014 = pd.read_excel('../Data/long/SABE_china_2011_2014.xlsx')\n",
    "sabe_china_2011_2014 = sabe_china_2011_2014.iloc[:,1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_china_2011_2014['edu_level'] = 0\n",
    "sabe_china_2011_2014.loc[(sabe_china_2011_2014['Education']==88) | (sabe_china_2011_2014['Education']==99), 'edu_level'] = 1\n",
    "sabe_china_2011_2014.loc[(sabe_china_2011_2014['Education']<=6), 'edu_level'] = 1\n",
    "sabe_china_2011_2014.loc[(sabe_china_2011_2014['Education']>=7) & (sabe_china_2011_2014['Education']<=12), 'edu_level'] = 2\n",
    "sabe_china_2011_2014.loc[(sabe_china_2011_2014['Education']>12), 'edu_level'] = 3\n",
    "\n",
    "sabe_china_2011_2014 = sabe_china_2011_2014.drop('Education', axis=1)\n",
    "\n",
    "sabe_china_2011_2014 = sabe_china_2011_2014.rename(columns={'edu_level': 'Education'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_col = list(sabe_chile.columns)\n",
    "del sabe_chile\n",
    "order_col\n",
    "\n",
    "order_col_long = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension', 'Heart Disease', 'Mental Problems', 'Physical activity', 'Alcohol consumption',\n",
    "       'Social activity', 'Smoking status', 'Income', 'Barthel_diff']\n",
    "order_col_2011 = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension', 'Heart Disease', 'Mental Problems', 'Physical activity', 'Alcohol consumption',\n",
    "       'Social activity', 'Smoking status', 'Income', 'Barthel_2011', 'Barthel_diff']\n",
    "order_col_2014 = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension', 'Heart Disease', 'Mental Problems', 'Physical activity', 'Alcohol consumption',\n",
    "       'Social activity', 'Smoking status', 'Income', 'Barthel_2014', 'Barthel_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_01 = sabe_costarica.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drop = ['FD_none_Edad_a01b', 'FD_none_Sexo_c18', 'FM_CardioMetab_Diabetes_c05', 'FS_Educ_yeduca', \n",
    "             'FS_Aislamiento_ViveSolo_g2',\n",
    "                'FM_CardioMetab_Hiperten_c04', 'FM_CardioMetab_IAM_c08',\n",
    "                 'FM_EstiloVida_Alcohol_c23', 'FM_EstiloVida_ActividadFis_c25a', 'FM_EstiloVida_Fuma_c24', \n",
    "                 'FM_EstiloVida_Caida12Mes_c11_med','FM_SaludMental_ProbNervDiagnost_c20', 'MMSE_diff', 'Barthel_diff']\n",
    "\n",
    "\n",
    "list_drop.append('Barthel_w2')\n",
    "list_drop.append('Barthel')\n",
    "X_costarica_01 = X_costarica_01[list_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_01_common_subjects = X_costarica_01.drop(['MMSE_diff' ], axis=1)\n",
    "X_costarica_01_common_subjects.drop(X_costarica_01_common_subjects[X_costarica_01_common_subjects['Barthel_diff'] <0].index, inplace=True)\n",
    "X_costarica_01_common_subjects = X_costarica_01_common_subjects.drop(['Barthel_diff' ], axis=1)\n",
    "X_costarica_01_common_subjects.dropna(inplace=True)\n",
    "\n",
    "\n",
    "print(X_costarica_01_common_subjects.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_w1 = X_costarica_01_common_subjects.drop(['Barthel_w2' ], axis=1)\n",
    "X_costarica_w2 = X_costarica_01_common_subjects.drop(['Barthel' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w1:', X_costarica_w1.shape[0], '\\w2:', X_costarica_w2.shape[0])\n",
    "print('w1:', X_costarica_01.shape[0] - X_costarica_w1.shape[0], \n",
    "      '\\tw2:', X_costarica_01_common_subjects.shape[0] - X_costarica_w2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_china_2011_2014_ = sabe_china_2011_2014[order_col_long]\n",
    "sabe_china_2011_ = sabe_china_2011_2014[order_col_2011]\n",
    "sabe_china_2014_ = sabe_china_2011_2014[order_col_2014]\n",
    "\n",
    "sabe_china_2011_ = sabe_china_2011_.rename(columns={'Barthel_2011': 'Barthel'})\n",
    "sabe_china_2014_ = sabe_china_2014_.rename(columns={'Barthel_2014': 'Barthel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension', 'Heart Disease', 'Mental Problems', 'Physical activity', 'Alcohol consumption',\n",
    "       'Social activity', 'Smoking status', 'Income', 'Barthel', 'Barthel_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_china_2011_.columns =  new_name\n",
    "sabe_china_2014_.columns =  new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_china_2011_['Barthel_diff'] = np.where(sabe_china_2011_['Barthel_diff'] < 0, np.nan, sabe_china_2011_['Barthel_diff'])\n",
    "sabe_china_2011_.dropna(inplace=True)\n",
    "\n",
    "sabe_china_2014_['Barthel_diff'] = np.where(sabe_china_2014_['Barthel_diff'] < 0, np.nan, sabe_china_2014_['Barthel_diff'])\n",
    "sabe_china_2014_.dropna(inplace=True)\n",
    "\n",
    "sabe_china_2011_ = sabe_china_2011_.drop(['Barthel_diff'], axis = 1)\n",
    "sabe_china_2014_ = sabe_china_2014_.drop(['Barthel_diff'], axis = 1)\n",
    "\n",
    "\n",
    "print(sabe_china_2011_.shape, sabe_china_2014_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors, variables names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = pd.read_csv('../Data/cross/var_name_color_code_new.csv', encoding='latin-1', sep=\";\")\n",
    "X_cat_china = pd.read_csv('../Data/cross/var_name_color_code_new_china.csv', encoding='latin-1', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = []\n",
    "\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    if(X_costarica_w1.columns[i] == 'FM_EstiloVida_Caida12Mes_c11_med'):\n",
    "        label = 'FM_EstiloVida_Caida12Mes_c11'\n",
    "    else:\n",
    "        label = X_costarica_w1.columns[i]\n",
    "    \n",
    "    index_ = list(X_cat.oldname).index(label)\n",
    "    new_name.append(list(X_cat.newname)[index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    print('', X_costarica_w1.columns[i], '\\n', X_costarica_w2.columns[i])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_w1.columns =  new_name\n",
    "X_costarica_w2.columns =  new_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    print('', X_costarica_w1.columns[i], '\\n', X_costarica_w2.columns[i])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_costarica_1 = X_costarica_w1['Barthel']\n",
    "X_costarica_1 = X_costarica_w1.drop(['Barthel'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_costarica_1, y_costarica_1, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_costarica_1 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_costarica_1.fit(X_costarica_1, y_costarica_1)\n",
    "\n",
    "parameter_dict['costarica_wave_1'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_costarica_1 = summary_validation( X_costarica_1, y_costarica_1, opt_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_costarica_2 = X_costarica_w2['Barthel']\n",
    "X_costarica_2 = X_costarica_w2.drop(['Barthel'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_costarica_2, y_costarica_2, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_costarica_2 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_costarica_2.fit(X_costarica_2, y_costarica_2)\n",
    "\n",
    "parameter_dict['costarica_wave_2'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_costarica_2 = summary_validation(X_costarica_2, y_costarica_2, opt_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_china_2011 = sabe_china_2011_['Barthel']\n",
    "X_china_2011 = sabe_china_2011_.drop('Barthel', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_china_2011, y_china_2011, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_china_2011 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_china_2011.fit(X_china_2011, y_china_2011)\n",
    "\n",
    "parameter_dict['china_2011'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_china_2011 = summary_validation(X_china_2011, y_china_2011, opt_Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_china_2014 = sabe_china_2014_['Barthel']\n",
    "X_china_2014 = sabe_china_2014_.drop('Barthel', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_china_2014, y_china_2014, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_china_2014 = Ridge(alpha=0.001, solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_china_2014.fit(X_china_2014, y_china_2014)\n",
    "\n",
    "parameter_dict['china_2014'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_china_2014 = summary_validation(X_china_2014, y_china_2014, opt_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension',\n",
    "       'Heart Disease', 'Alcohol consumption', 'Physical activity',\n",
    "       'Smoking status', 'Mental Problems']\n",
    "\n",
    "df = modelRidge_sum_costarica_2\n",
    "\n",
    "feature_sign = []\n",
    "\n",
    "for i in col:\n",
    "    if df[0].loc[i, 'p value stouffer'] < 0.05:\n",
    "        feature_sign.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CR_c = pd.DataFrame(modelRidge_sum_costarica_2[1])\n",
    "df_CH_c = pd.DataFrame(modelRidge_sum_china_2014[1])\n",
    "\n",
    "df_CR_p = pd.DataFrame(modelRidge_sum_costarica_2[2])\n",
    "df_CH_p = pd.DataFrame(modelRidge_sum_china_2014[2])\n",
    "\n",
    "\n",
    "df_CR_t = pd.DataFrame(modelRidge_sum_costarica_2[3])\n",
    "df_CH_t = pd.DataFrame(modelRidge_sum_china_2014[3])\n",
    "\n",
    "\n",
    "df_CR_c['Countries'] = 'CR'\n",
    "df_CH_c['Countries'] = 'CH'\n",
    "\n",
    "df_CR_p['Countries'] = 'CR'\n",
    "df_CH_p['Countries'] = 'CH'\n",
    "\n",
    "df_CR_t['Countries'] = 'CR'\n",
    "df_CH_t['Countries'] = 'CH'\n",
    "\n",
    "df_c = pd.concat([df_CR_c, df_CH_c])\n",
    "df_p = pd.concat([df_CR_p, df_CH_p])\n",
    "df_t = pd.concat([df_CR_t, df_CH_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df_c[feature_sign + ['Countries']]\n",
    "df_p = df_p[feature_sign + ['Countries']]\n",
    "df_t = df_t[feature_sign + ['Countries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from scipy import stats\n",
    "\n",
    "for feat in feature_sign:\n",
    "    current_feat = df_t[feat]\n",
    "    current_feat.dropna(inplace=True)\n",
    "    stat, p = shapiro(current_feat)\n",
    "    kurt = stats.kurtosis(current_feat)\n",
    "    skew = stats.skew(current_feat)\n",
    "\n",
    "    \n",
    "    feat_name = feat.ljust(45)\n",
    "    print('{} \\t p-value: {} \\tKursotis: {}  \\tSkewness: {}'.format(feat_name, np.round(p,4), np.round(kurt,4), np.round(skew,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['CR', 'CH']\n",
    "plt.figure(figsize= (11,9))\n",
    "plt.suptitle('Mann-Whitney estimate functionality');\n",
    "#hue = \"Countries\"\n",
    "\n",
    "my_colors = [\"#222222\",  \"#999999\"]\n",
    "\n",
    "c = 1\n",
    "for current_feat in feature_sign:\n",
    "    plt.subplot(3,3,c)\n",
    "   \n",
    "    p_cr  = modelRidge_sum_costarica_2[0].loc[current_feat, 'p value stouffer']\n",
    "    p_ch  = modelRidge_sum_china_2014[0].loc[current_feat, 'p value stouffer']\n",
    "        \n",
    "    df_c_ = df_c.copy()    \n",
    "    \n",
    "    comp = [(\"CR\", \"CH\")]\n",
    "    \n",
    "    do_comp = True\n",
    "    \n",
    "        \n",
    "    if(p_ch > 0.05):    \n",
    "        df_c_[df_c_['Countries'] == 'CH']=-0\n",
    "\n",
    "        do_comp = False\n",
    "    \n",
    "    ax = sns.violinplot(data=df_c_, x='Countries', y=current_feat, palette= my_colors)\n",
    "\n",
    "\n",
    "    \n",
    "    if(do_comp):\n",
    "        test_results = add_stat_annotation(ax, data=df_c, x='Countries', y=current_feat, order=order,\n",
    "                                            box_pairs=comp,\n",
    "                                            test='Mann-Whitney', text_format='star',\n",
    "                                            loc='inside', verbose=0)\n",
    "    \n",
    "    c+=1\n",
    "    \n",
    "    plt.xlim([-0.5, 1.5])\n",
    "    plt.xticks([0,1], ['CR','CH'])\n",
    "    plt.tight_layout(pad=3);\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistically significant variables in  Costa Rica wave 2, Korea 2016 and China 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_test = ['Age', 'Sex', 'Diabetes', 'Education', 'Hypertension',\n",
    "       'Heart Disease', 'Alcohol consumption', 'Physical activity',\n",
    "       'Smoking status', 'Mental Problems']\n",
    "\n",
    "\n",
    "df_CR = pd.DataFrame(modelRidge_sum_costarica_2[1])\n",
    "df_CR = df_CR[features_for_test]\n",
    "df_CR.columns = [f\"CR_{col}\" for col in df_CR.columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_CH = pd.DataFrame(modelRidge_sum_china_2014[1])\n",
    "df_CH = df_CH[features_for_test]\n",
    "df_CH.columns = [f\"CH_{col}\" for col in df_CH.columns]\n",
    "\n",
    "\n",
    "\n",
    "beta_df = pd.concat([df_CR, df_CH], axis=1)\n",
    "\n",
    "\n",
    "result_stats_df = pd.DataFrame(\n",
    "        index=['CH mean', 'CR mean', 'CR_CH test_p value'] ,\n",
    "        columns=features_for_test\n",
    "        )\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api\n",
    "\n",
    "for i in features_for_test:\n",
    "\n",
    "\n",
    "    \n",
    "    result_stats_df.loc['CR_CH test_p value',i] = np.min([statsmodels.stats.weightstats.ttest_ind(list(beta_df.loc[:, 'CR_'+ i]), \n",
    "                                                                                          list(beta_df.loc[:, 'CH_'+ i]), \n",
    "                                                                                          alternative=\"two-sided\",\n",
    "                                                                                          usevar=\"unequal\")[1]*3,1])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    result_stats_df.loc['CH mean',i] = beta_df.loc[:, 'CH_'+ i].mean()\n",
    "    result_stats_df.loc['CR mean',i] = beta_df.loc[:, 'CR_'+ i].mean()\n",
    "result_stats_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_costarica_1[0]#.to_excel('Results/longitudinal/costarica_wave1_func.xlsx')\n",
    "modelRidge_sum_costarica_2[0]#.to_excel('Results/longitudinal/costarica_wave2_func.xlsx')\n",
    "\n",
    "modelRidge_sum_china_2011[0]#.to_excel('Results/longitudinal/china_wave1_func.xlsx')\n",
    "modelRidge_sum_china_2014[0]#.to_excel('Results/longitudinal/china_wave2_func.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_pd = pd.DataFrame(parameter_dict).T\n",
    "parameter_pd#.to_excel('Results/longitudinal/functionality_hyperparameters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_stats_df.T#.to_excel('Results/longitudinal/stats_func.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
