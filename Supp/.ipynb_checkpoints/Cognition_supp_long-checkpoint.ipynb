{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.font_manager\n",
    "from matplotlib import style\n",
    "style.use('seaborn') or plt.style.use('seaborn')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "import scipy.stats as ss\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Lasso, MultiTaskLasso, Ridge, ElasticNet\n",
    "import math\n",
    "\n",
    "#from regressors import stats\n",
    "\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import colorsys\n",
    "import matplotlib.colors as cconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_bib as mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(var, X_cat_, color_dict_):\n",
    "\n",
    "    X_factors = list(X_cat_.factors)\n",
    "    X_name = list(X_cat_.newname)\n",
    "\n",
    "    index_code = X_name.index(var)\n",
    "    cat = X_factors[index_code]\n",
    "\n",
    "    return color_dict_[cat]\n",
    "\n",
    "color_dict = {}\n",
    "color_dict['Country'] = [40/255, 43/255, 95/255]\n",
    "color_dict['HF'] = '#A31300'\n",
    "color_dict['LSF'] = '#FF9E0D'\n",
    "color_dict['PSF'] = '#FF4800'\n",
    "color_dict['SDHF'] = '#1C4F9E'\n",
    "color_dict['DF'] = '#009E32'\n",
    "\n",
    "def get_bar_colors(data_, X_cat_):\n",
    "    color_dict = {}\n",
    "    color_dict['Country'] = [40/255, 43/255, 95/255]\n",
    "    color_dict['HF'] = '#A31300'\n",
    "    color_dict['LSF'] = '#FF9E0D'\n",
    "    color_dict['PSF'] = '#FF4800'\n",
    "    color_dict['SDHF'] = '#1C4F9E'\n",
    "    color_dict['DF'] = '#009E32'\n",
    "    \n",
    "    #color_dict = {}\n",
    "    #color_dict = {'DF':'#5975a4', 'MF':'#cc8963', 'SF':'#5f9e6e', 'SLF':'#b55d60',\n",
    "    #              'n':'#857aab', 'country':'#8d7866'}#, '#d095bf'}\n",
    "\n",
    "    X_factors = list(X_cat_.factors)\n",
    "    X_name = list(X_cat_.newname)\n",
    "\n",
    "    bar_color_total = []\n",
    "    for i in list(data_.Features):\n",
    "        if(i == 'Country'):\n",
    "            bar_color_total.append(color_dict['Country'])\n",
    "            continue\n",
    "        index_code = X_name.index(i)\n",
    "        cat = X_factors[index_code]\n",
    "        bar_color_total.append(color_dict[cat])\n",
    "    return bar_color_total\n",
    "\n",
    "\n",
    "def plot_estimate_value(regression_model, X_cat_ = [], title = '',  xlim =[0, 2] ,fig_size = (8,12), size = 14, pvalue_type = 'False'):\n",
    "\n",
    "    df = regression_model[0]\n",
    "    df.index.name = 'Features'\n",
    "    df = df.iloc[1:-3, 0:-1]\n",
    "\n",
    "    for i in range(3):\n",
    "            #print(i, X_RAW_edu_level[X_RAW_edu_level.columns[i]].dtype)\n",
    "        df[df.columns[i]] = np.abs(pd.to_numeric(df[df.columns[i]],errors = 'coerce'))\n",
    "\n",
    "    df = df.reset_index()\n",
    "    data = df.sort_values('Estimate', ascending=False)\n",
    "\n",
    "    \n",
    "    if(len(X_cat_)>0):\n",
    "        \n",
    "        bar_color = get_bar_colors(data, X_cat_)\n",
    "        \n",
    "        plt.title(title)\n",
    "        sns.barplot(x=\"Estimate\", y=\"Features\", data = data, palette =bar_color)\n",
    "        plt.xlim(xlim)\n",
    "    else:\n",
    "        plt.title(title)\n",
    "        sns.barplot(x=\"Estimate\", y=\"Features\", data = data, color = 'darkblue')\n",
    "        plt.xlim(xlim)\n",
    "\n",
    "\n",
    "    y_step=0  \n",
    "    for i in range(df.shape[0]):\n",
    "        if(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.01):\n",
    "            color = 'green'\n",
    "        elif(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.05):\n",
    "            color = 'gray'\n",
    "        else:\n",
    "            color = 'red'        \n",
    "        \n",
    "        if(pvalue_type == 'color'):\n",
    "                plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]-0.005, y_step, \n",
    "                                 '' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],2)),\n",
    "                                 size= size, rotation=0.,\n",
    "                                 ha=\"left\", va=\"center\", color = 'white',\n",
    "                                 bbox=dict(boxstyle=\"round\",\n",
    "                                           ec=color,\n",
    "                                            fc=color,\n",
    "                                           )\n",
    "                                 )\n",
    "                y_step+=1\n",
    "\n",
    "        elif(pvalue_type == 'value'):\n",
    "                plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                 '(' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],8))+')',\n",
    "                                 size= size, rotation=0.,\n",
    "                                 ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                 )\n",
    "                y_step+=1\n",
    "        else:\n",
    "            if(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]<0.01):\n",
    "                \n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '**',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )\n",
    "            elif(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]>= 0.01 and df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step]<0.05):\n",
    "                  \n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '*',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )  \n",
    "            else:\n",
    "                    plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]+0.005, y_step, \n",
    "                                     '',\n",
    "                                     size= size, rotation=0.,\n",
    "                                     ha=\"left\", va=\"center\", color = 'black',\n",
    "\n",
    "                                     )      \n",
    "            y_step+=1\n",
    "            \n",
    "         \n",
    "    text_diff =xlim[1]/2.2\n",
    "    plt.text(xlim[1] - text_diff, df.shape[0]-1.5,r'$ R^2 $(' + str(np.round(regression_model[1],2)) + ') \\t$F^2 $(' + str(np.round(regression_model[3],2)) + ')',\n",
    "                             size= 12, rotation=0.,\n",
    "                             ha=\"left\", va=\"center\", color = 'black',\n",
    "                             bbox=dict(boxstyle=\"round\",\n",
    "                                       ec='gray',\n",
    "                                        fc='gray',\n",
    "                                       )\n",
    "                             )\n",
    "\n",
    "\n",
    "    plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "\n",
    "def plot_estimate_value_no_sort(regression_model, title = '',  xlim =[0, 2] ,fig_size = (8,12), size = 14, ylabel = True, ylabelR = False ):\n",
    "\n",
    "    df = regression_model\n",
    "    df.index.name = 'Features'\n",
    "    df = df.iloc[1:-3, 0:-1]\n",
    "\n",
    "    for i in range(3):\n",
    "            #print(i, X_RAW_edu_level[X_RAW_edu_level.columns[i]].dtype)\n",
    "        df[df.columns[i]] = np.abs(pd.to_numeric(df[df.columns[i]],errors = 'coerce'))\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    #plt.figure(figsize=(8,12))\n",
    "    plt.title(title)\n",
    "    sns.barplot(x=\"Estimate\", y=\"Features\", data = df, color = 'Brown')\n",
    "    if(ylabel == False):\n",
    "        plt.ylabel('')\n",
    "        plt.yticks([])\n",
    "    plt.xlim(xlim)\n",
    "    \n",
    "    if(ylabelR):\n",
    "        plt.tick_params (axis = 'y', which = 'both', labelleft = False, labelright = True)\n",
    "\n",
    "    y_step=0  \n",
    "    for i in range(df.shape[0]):\n",
    "        if(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.01):\n",
    "            color = 'green'\n",
    "        elif(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)<=0.05):\n",
    "            color = 'gray'\n",
    "        else:\n",
    "            color = 'red'        \n",
    "        \n",
    "        plt.text(df.sort_values('Estimate', ascending=False)['Estimate'].iloc[y_step]-0.005, y_step, \n",
    "                         '' + str(np.round(df.sort_values('Estimate', ascending=False)['p value'].iloc[y_step],3)),\n",
    "                         size= size, rotation=0.,\n",
    "                         ha=\"left\", va=\"center\", color = 'white',\n",
    "                         bbox=dict(boxstyle=\"round\",\n",
    "                                   ec=color,\n",
    "                                    fc=color,\n",
    "                                   )\n",
    "                         )\n",
    "        y_step+=1\n",
    "        \n",
    "\n",
    "def conf_interval(database):\n",
    "    r_squared_list = []\n",
    "\n",
    "    y_database = database['MMSE']\n",
    "    X_database = database.drop('MMSE', axis=1)\n",
    "\n",
    "    for i in range(0,100,10):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "        opt_Ridge = BayesSearchCV(\n",
    "            Ridge(),\n",
    "            {\n",
    "                'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                'max_iter': (1000, 10000, 100000, 1000000),\n",
    "            },\n",
    "            n_iter=10,\n",
    "            random_state=i, \n",
    "            scoring='r2',\n",
    "            cv=3\n",
    "        )\n",
    "\n",
    "        opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "        r_squared_list.append(opt_Ridge.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    print(r'99% confidence interval (+-', np.round(np.std(r_squared_list)*2.58, 4),')')\n",
    "    return np.round(np.std(r_squared_list)*2.58, 4)\n",
    "\n",
    "        \n",
    "def adj_r2_score_and_r2_score(clf, X, y):\n",
    "    n = X.shape[0]  # Number of observations\n",
    "    p = X.shape[1]  # Number of features\n",
    "    r_squared = r2_score(y, clf.predict(X))\n",
    "    return [1 - (1 - r_squared) * ((n - 1) / (n - p - 1)), r_squared]\n",
    "\n",
    "\n",
    "def mse(clf, X, y):\n",
    "    return mean_squared_error(y, clf.predict(X))\n",
    "\n",
    "def rmse(clf, X, y):\n",
    "    mse = mean_squared_error(y, clf.predict(X))\n",
    "    return math.sqrt(mse)    \n",
    "\n",
    "def coef_se(clf, X, y):\n",
    "    n = X.shape[0]\n",
    "    X1 = np.hstack((np.ones((n, 1)), np.matrix(X)))\n",
    "    se_matrix = scipy.linalg.sqrtm(\n",
    "        metrics.mean_squared_error(y, clf.predict(X)) *\n",
    "        np.linalg.inv(X1.T * X1)\n",
    "    )\n",
    "    return np.diagonal(se_matrix)\n",
    "\n",
    "def coef_tval(clf, X, y):\n",
    "    a = np.array(clf.intercept_ / coef_se(clf, X, y)[0])\n",
    "    b = np.array(clf.coef_ / coef_se(clf, X, y)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "def coef_tval_XGB_tree(clf, X, y):\n",
    "    a = np.nan\n",
    "    b = np.array(clf.feature_importances_ / coef_se(clf, X, y)[1:])\n",
    "    return np.append(a, b)\n",
    "\n",
    "def coef_pval(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    t = coef_tval(clf, X, y)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "def coef_pval_XGB_tree(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    t = coef_tval_XGB_tree(clf, X, y)\n",
    "    p = 2 * (1 - scipy.stats.t.cdf(abs(t), n - 1))\n",
    "    return p\n",
    "\n",
    "def residuals(clf, X, y, r_type='standardized'):\n",
    "\n",
    "    # Make sure value of parameter 'r_type' is one we recognize\n",
    "    assert r_type in ('raw', 'standardized', 'studentized'), (\n",
    "        \"Invalid option for 'r_type': {0}\".format(r_type))\n",
    "    y_true = y.view(dtype='float')\n",
    "    # Use classifier to make predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    # Make sure dimensions agree (Numpy still allows subtraction if they don't)\n",
    "    assert y_true.shape == y_pred.shape, (\n",
    "        \"Dimensions of y_true {0} do not match y_pred {1}\".format(y_true.shape,\n",
    "                                                                  y_pred.shape))\n",
    "    # Get raw residuals, or standardized or standardized residuals\n",
    "    resids = y_pred - y_true\n",
    "    if r_type == 'standardized':\n",
    "        resids = resids / np.std(resids)\n",
    "    elif r_type == 'studentized':\n",
    "        # Prepare a blank array to hold studentized residuals\n",
    "        studentized_resids = np.zeros(y_true.shape[0], dtype='float')\n",
    "        # Calcluate hat matrix of X values so you can get leverage scores\n",
    "        hat_matrix = np.dot(\n",
    "            np.dot(X, np.linalg.inv(np.dot(np.transpose(X), X))),\n",
    "            np.transpose(X))\n",
    "        # For each point, calculate studentized residuals w/ leave-one-out MSE\n",
    "        for i in range(y_true.shape[0]):\n",
    "            # Make a mask so you can calculate leave-one-out MSE\n",
    "            mask = np.ones(y_true.shape[0], dtype='bool')\n",
    "            mask[i] = 0\n",
    "            loo_mse = np.average(resids[mask] ** 2, axis=0)  # Leave-one-out MSE\n",
    "            # Calculate studentized residuals\n",
    "            studentized_resids[i] = resids[i] / np.sqrt(\n",
    "                loo_mse * (1 - hat_matrix[i, i]))\n",
    "        resids = studentized_resids\n",
    "    return resids\n",
    "\n",
    "\n",
    "def f_squared(clf, X, y):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    return r_squared  / (1 - r_squared)\n",
    "\n",
    "\n",
    "\n",
    "def summary(clf, X, y, xlabels=None, regressor = ''):\n",
    "\n",
    "    print('Resumen del regresor ' + regressor + '\\n')\n",
    "    \n",
    "    ncols = X.shape[1]\n",
    "    if xlabels is None:\n",
    "        xlabels = np.array(\n",
    "            ['x{0}'.format(i) for i in range(1, ncols + 1)], dtype='str')\n",
    "    elif isinstance(xlabels, (tuple, list)):\n",
    "        xlabels = np.array(xlabels, dtype='str')\n",
    "\n",
    "    # Create data frame of coefficient estimates and associated stats\n",
    "    coef_df = pd.DataFrame(\n",
    "        index=['_intercept'] + list(xlabels),\n",
    "        columns=['Estimate','t value', 'p value']\n",
    "    )\n",
    "    \n",
    "    if(regressor == 'XGBRegressor'):\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "            (np.round(np.array([clf.intercept_[0]]), 6), np.round((clf.coef_), 6)))\n",
    "        #coef_df['MSE'] = np.round(mse(clf, X, y), 6)\n",
    "        #coef_df['RMSE'] = np.round(rmse(clf, X, y), 6)\n",
    "        coef_df['t value'] = np.round(coef_tval(clf, X, y), 4)\n",
    "        coef_df['p value'] = np.round(coef_pval(clf, X, y), 20)\n",
    "        # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "            'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "            '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "            'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "            '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "            'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "        # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "        \n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "           r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "            f_sq))\n",
    "    elif(regressor == 'XGBRegressorNoLinear'):\n",
    "        coef_df = pd.DataFrame(\n",
    "            index=['_intercept'] + list(xlabels),\n",
    "            columns=['Estimate','t value', 'p value']\n",
    "        )\n",
    "\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "                (np.round(np.array([np.nan]), 6), np.round((clf.feature_importances_), 6)))\n",
    "\n",
    "        coef_df['t value'] = np.round(coef_tval_XGB_tree(clf, X, y), 4)\n",
    "        coef_df['p value'] = np.round(coef_pval_XGB_tree(clf, X, y), 20)\n",
    "            # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "                'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "                '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "                'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "                '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "                'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "            # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "\n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "               r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "                f_sq))\n",
    "    else:\n",
    "        coef_df['Estimate'] = np.concatenate(\n",
    "            (np.round(np.array([clf.intercept_]), 6), np.round((clf.coef_), 6)))\n",
    "        #coef_df['MSE'] = np.round(mse(clf, X, y), 6)\n",
    "        #coef_df['RMSE'] = np.round(rmse(clf, X, y), 6)\n",
    "        coef_df['t value'] = abs(np.round(coef_tval(clf, X, y), 4))\n",
    "        coef_df['p value'] = np.round(coef_pval(clf, X, y), 20)\n",
    "        # Create data frame to summarize residuals\n",
    "        resids = residuals(clf, X, y, r_type='raw')\n",
    "        resids_df = pd.DataFrame({\n",
    "            'Min': pd.Series(np.round(resids.min(), 4)),\n",
    "            '1Q': pd.Series(np.round(np.percentile(resids, q=25), 4)),\n",
    "            'Median': pd.Series(np.round(np.median(resids), 4)),\n",
    "            '3Q': pd.Series(np.round(np.percentile(resids, q=75), 4)),\n",
    "            'Max': pd.Series(np.round(resids.max(), 4)),\n",
    "        }, columns=['Min', '1Q', 'Median', '3Q', 'Max'])\n",
    "        # Output results\n",
    "        print(\"Residuals:\")\n",
    "        print(resids_df.to_string(index=False))\n",
    "        print('\\n')\n",
    "        print('Coefficients:')\n",
    "        print(coef_df.to_string(index=True))\n",
    "        print('---')\n",
    "        \n",
    "        r_sq = adj_r2_score_and_r2_score(clf, X, y)[1]\n",
    "        r_sq_adj = adj_r2_score_and_r2_score(clf, X, y)[0]\n",
    "        f_sq = f_squared(clf, X, y)\n",
    "        \n",
    "        print('R-squared:  {0:.5f},    Adjusted R-squared:  {1:.5f}'.format(\n",
    "           r_sq, r_sq_adj))\n",
    "        print('F-squared:  {0:.5f}'.format(\n",
    "            f_sq))\n",
    "        \n",
    "        \n",
    "    print('---------------------------------------------------------------------------\\n\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    empty_str = []\n",
    "    for i in range(coef_df.shape[0]):\n",
    "        empty_str.append('')\n",
    "    \n",
    "    coef_df['value'] = empty_str\n",
    "    \n",
    "    coef_df = coef_df.T\n",
    "    coef_df['R-squared'] = ['','','', r_sq]\n",
    "    coef_df['Adjusted R-squared'] = ['','','', r_sq_adj]\n",
    "    coef_df['F-squared'] = ['','','', f_sq]\n",
    "    return [coef_df.T, r_sq, r_sq_adj, f_sq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    sig = 1 / (1 + math.exp(-x))\n",
    "    return sig\n",
    "\n",
    "def get_order_var(model_):\n",
    "\n",
    "    df = model_[0]['Estimate'].iloc[1:-3]\n",
    "    df = np.abs(pd.to_numeric(df,errors = 'coerce'))\n",
    "    df = list(df.sort_values(ascending=False).index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_longitudinal(var, y_countrys_, models_, market_size_type = 'log',xlim = [0.25, 5.75], \n",
    "                      years = [2006, 2014, 2016],ylim = [10.0, 11.12], size_init = 1, size_mult = 40, \n",
    "                      size_edge_market = 2, xlabel = True, ylabel = True, ylabel_text = 'MMSE',color_dict_ = '', X_cat_ = ''):\n",
    "    \n",
    "    #X_cat = pd.read_csv('Data/var_name_color_code.csv', encoding='latin-1', sep=\";\")\n",
    "    \n",
    "    color = get_color(var, X_cat_, color_dict_)\n",
    "    edge_color = 'black'\n",
    "\n",
    "    y = []\n",
    "    for i in range(len(y_countrys_)):\n",
    "        y.append(y_countrys_[i].mean())\n",
    "        #y = [y_countrys_[0].mean(), y_countrys_[1].mean(),\n",
    "        #     y_countrys_[2].mean(), y_countrys_[3].mean(),\n",
    "        #     y_countrys_[4].mean()]\n",
    "\n",
    "\n",
    "    #var_ind = list(X_cat.oldname).index(var)\n",
    "    #var_ = list(X_cat.newname)[var_ind]\n",
    "    \n",
    "\n",
    "     \n",
    "    plt.plot(range(1, len(y_countrys_)+1), y, 'k--')\n",
    "\n",
    "    for i in range(len(models_)):\n",
    "        df = models_[i][0]\n",
    "        y = np.abs(df.iloc[df.index.get_loc(var), df.columns.get_loc('Estimate')])\n",
    "        \n",
    "        if(market_size_type=='log'):\n",
    "            if(y < 0.003):\n",
    "                markersize_ = 0\n",
    "            else:\n",
    "                markersize_ = size_mult*(5 + np.log(y))\n",
    "        elif(market_size_type=='log1'):\n",
    "            markersize_ = np.log(100000*y)\n",
    "        elif(market_size_type=='sigmoide'):\n",
    "            markersize_ = size_mult*sigmoid(y)\n",
    "        else:\n",
    "            markersize_ = np.abs(size_mult*y) \n",
    "        \n",
    "        \n",
    "        markersize_ +=size_init\n",
    "        plt.plot(i + 1, y_countrys_[i].mean(), marker=\"o\",  \n",
    "                 markeredgewidth = size_edge_market, markeredgecolor = edge_color, markerfacecolor = color, markersize= markersize_)\n",
    "        \n",
    "        \n",
    "        p = df['p value'][var]\n",
    "        \n",
    "        p_text = ''\n",
    "        if(p<= 0.01):\n",
    "            p_text = '**'\n",
    "        elif((p<= 0.05)):\n",
    "            p_text = '*'\n",
    "        \n",
    "        \n",
    "        plt.text(i + 1 , y_countrys_[i].mean(), p_text,\n",
    "                                             size= 22, rotation=0.,\n",
    "                                             ha=\"center\", va=\"top\", color = 'black',\n",
    "                                            # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                            #           ec='gray',\n",
    "                                            #           fc='gray',\n",
    "                                            #          )\n",
    "                                             );\n",
    "        \n",
    "       # plt.text(i + 1 , ylim[0] - 0.25, str(round(markersize_,2)),\n",
    "        #                                     size= 8, rotation=0.,\n",
    "         #                                    ha=\"center\", va=\"top\", color = 'black',\n",
    "                                            # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                            #           ec='gray',\n",
    "                                            #           fc='gray',\n",
    "                                            #          )\n",
    "          #                                   );\n",
    "       # print('')\n",
    "        r2 = str(np.round(models_[i][1],2))\n",
    "        f2 = str(np.round(models_[i][3],2))\n",
    "        #plt.text(i+1,10.15,r'$R^2$ ('+r2+') \\n$F^2$ ('+f2+')',\n",
    "        #                         size= 12, rotation=0.,\n",
    "        #                         ha=\"center\", va=\"top\", color = 'black',\n",
    "        #                         bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "        #                                   ec='ghostwhite',\n",
    "        #                                    fc='ghostwhite',\n",
    "        #                                  )\n",
    "        #                         );\n",
    "\n",
    "    plt.text(xlim[1] , ylim[1], var,\n",
    "                                         size= 13, rotation=0.,\n",
    "                                         ha=\"right\", va=\"center\", color = 'black',\n",
    "                                        # bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                        #           ec='gray',\n",
    "                                        #           fc='gray',\n",
    "                                        #          )\n",
    "                                         );\n",
    "\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim([ylim[0]-0.25, ylim[1]+0.25])\n",
    "    \n",
    "\n",
    "    if(xlabel):\n",
    "        plt.xticks(list(range(1, len(years)+1)), years)\n",
    "    else:\n",
    "        plt.xticks(list(range(1, len(years)+1)), ['', '', '', '', ''])\n",
    "    \n",
    "    if(ylabel):\n",
    "        plt.yticks(np.round(np.linspace(ylim[0], ylim[1], 4),2), np.round(np.linspace(ylim[0], ylim[1], 4),1))\n",
    "    else:\n",
    "        plt.yticks(np.round(np.linspace(ylim[0], ylim[1], 4),2), ['', '', '', ''])\n",
    "\n",
    "    if(ylabel):\n",
    "        plt.ylabel(ylabel_text, fontsize=13)\n",
    "    else:\n",
    "        plt.ylabel('')\n",
    "        \n",
    "    if(xlabel):\n",
    "        plt.xlabel('Years', fontsize=13)\n",
    "    else:\n",
    "        plt.xlabel('')\n",
    "\n",
    "def plot_r2_f2(models_, xlim = [0.25, 5.75], ylim = [10.0, 11.12]):\n",
    "    \n",
    "    X_cat = pd.read_csv('Data/var_name_color_code.csv', encoding='latin-1', sep=\";\")\n",
    "    \n",
    "    \n",
    "    plt.plot(0.0)\n",
    "\n",
    "    for i in range(len(models)):\n",
    "\n",
    "        r2 = str(np.round(models_[i][1],2))\n",
    "        f2 = str(np.round(models_[i][3],2))\n",
    "        plt.text(i+1,1,r'$R^2$ ('+r2+') \\n$F^2$ ('+f2+')',\n",
    "                                 size= 12, rotation=0.,\n",
    "                                 ha=\"center\", va=\"top\", color = 'black',\n",
    "                                 #bbox=dict(boxstyle=\"round\", pad=0.1,\n",
    "                                 #          ec='ghostwhite',\n",
    "                                 #           fc='ghostwhite',\n",
    "                                 #         )\n",
    "                                 );\n",
    "        \n",
    "    \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim([ylim[0], ylim[1]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabe_chile = pd.read_excel('../Data/cross/SABE_chile.xlsx') # to use it for columns order\n",
    "sabe_chile = sabe_chile.iloc[:,1::]\n",
    "\n",
    "sabe_costarica = pd.read_excel('../Data/long/SABE_costarica_long.xlsx')\n",
    "sabe_costarica = sabe_costarica.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006 = pd.read_excel('../Data/long/SABE_korea_2006.xlsx')\n",
    "sabe_korea_2006 = sabe_korea_2006.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006_2008 = pd.read_excel('../Data/long/SABE_korea_2006_2008.xlsx')\n",
    "sabe_korea_2006_2008 = sabe_korea_2006_2008.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006_2010 = pd.read_excel('../Data/long/SABE_korea_2006_2010.xlsx')\n",
    "sabe_korea_2006_2010 = sabe_korea_2006_2010.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006_2012 = pd.read_excel('../Data/long/SABE_korea_2006_2012.xlsx')\n",
    "sabe_korea_2006_2012 = sabe_korea_2006_2012.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006_2014 = pd.read_excel('../Data/long/SABE_korea_2006_2014.xlsx')\n",
    "sabe_korea_2006_2014 = sabe_korea_2006_2014.iloc[:,1::]\n",
    "\n",
    "sabe_korea_2006_2016 = pd.read_excel('../Data/long/SABE_korea_2006_2016.xlsx')\n",
    "sabe_korea_2006_2016 = sabe_korea_2006_2016.iloc[:,1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_col = list(sabe_chile.columns)\n",
    "del sabe_chile\n",
    "order_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_01 = sabe_costarica.copy()\n",
    "\n",
    "X_korea_2006_01 = sabe_korea_2006.copy()\n",
    "X_korea_2006_2008_01 = sabe_korea_2006_2008.copy()\n",
    "X_korea_2006_2010_01 = sabe_korea_2006_2010.copy()\n",
    "X_korea_2006_2012_01 = sabe_korea_2006_2012.copy()\n",
    "X_korea_2006_2014_01 = sabe_korea_2006_2014.copy()\n",
    "X_korea_2006_2016_01 = sabe_korea_2006_2016.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drop = ['CASEID', 'FD_none_Edad_a01b', 'FD_none_Sexo_c18', 'FM_CardioMetab_Diabetes_c05', 'FS_Educ_yeduca', \n",
    "             'FS_Aislamiento_ViveSolo_g2_med',\n",
    "                'FM_CardioMetab_Hiperten_c04', 'FM_CardioMetab_IAM_c08',\n",
    "                 'FM_EstiloVida_Alcohol_c23', 'FM_EstiloVida_ActividadFis_c25a', 'FM_EstiloVida_Fuma_c24', \n",
    "                 'FM_EstiloVida_Caida12Mes_c11', 'FM_SaludMental_ProbNervDiagnost_c20', 'MMSE_diff', 'Barthel_diff']\n",
    "\n",
    "list_drop.append('MMSE_2008')\n",
    "X_korea_2006_2008_01 = X_korea_2006_2008_01[list_drop]\n",
    "\n",
    "list_drop.remove('MMSE_2008')\n",
    "list_drop.append('MMSE_2010')\n",
    "X_korea_2006_2010_01 = X_korea_2006_2010_01[list_drop]\n",
    "\n",
    "list_drop.remove('MMSE_2010')\n",
    "list_drop.append('MMSE_2012')\n",
    "X_korea_2006_2012_01 = X_korea_2006_2012_01[list_drop]\n",
    "\n",
    "list_drop.remove('MMSE_2012')\n",
    "list_drop.append('MMSE_2014')\n",
    "X_korea_2006_2014_01 = X_korea_2006_2014_01[list_drop]\n",
    "\n",
    "list_drop.remove('MMSE_2014')\n",
    "list_drop.append('MMSE_2016')\n",
    "X_korea_2006_2016_01 = X_korea_2006_2016_01[list_drop]\n",
    "\n",
    "\n",
    "list_drop = ['FD_none_Edad_a01b', 'FD_none_Sexo_c18', 'FM_CardioMetab_Diabetes_c05', 'FS_Educ_yeduca', \n",
    "             'FS_Aislamiento_ViveSolo_g2',\n",
    "                'FM_CardioMetab_Hiperten_c04', 'FM_CardioMetab_IAM_c08',\n",
    "                 'FM_EstiloVida_Alcohol_c23', 'FM_EstiloVida_ActividadFis_c25a', 'FM_EstiloVida_Fuma_c24', \n",
    "                 'FM_EstiloVida_Caida12Mes_c11_med','FM_SaludMental_ProbNervDiagnost_c20', 'MMSE_diff', 'Barthel_diff']\n",
    "\n",
    "\n",
    "list_drop.append('MMSE_w2')\n",
    "list_drop.append('MMSE')\n",
    "X_costarica_01 = X_costarica_01[list_drop]\n",
    "\n",
    "\n",
    "list_drop = ['CASEID', 'FD_none_Edad_a01b', 'FD_none_Sexo_c18', 'FM_CardioMetab_Diabetes_c05', 'FS_Educ_yeduca', 'FS_Aislamiento_ViveSolo_g2_med',\n",
    "                'FM_CardioMetab_Hiperten_c04', 'FM_CardioMetab_IAM_c08',\n",
    "                 'FM_EstiloVida_Alcohol_c23', 'FM_EstiloVida_ActividadFis_c25a', 'FM_EstiloVida_Fuma_c24', \n",
    "                 'FM_EstiloVida_Caida12Mes_c11','FM_SaludMental_ProbNervDiagnost_c20', 'MMSE']\n",
    "\n",
    "\n",
    "X_korea_2006_01 = X_korea_2006_01[list_drop]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data and get common participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_common_subjects = X_korea_2006_01.copy()\n",
    "X_korea_2006_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_ids = X_korea_2006_common_subjects['CASEID']\n",
    "print(X_korea_2006_common_subjects.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2008_common_subjects = X_korea_2006_2008_01.drop(['Barthel_diff'], axis=1)\n",
    "X_korea_2006_2008_common_subjects.drop(X_korea_2006_2008_common_subjects[X_korea_2006_2008_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_korea_2006_2008_common_subjects = X_korea_2006_2008_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_korea_2006_2008_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_2008_ids = X_korea_2006_2008_common_subjects['CASEID']\n",
    "print(X_korea_2006_2008_common_subjects.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2010_common_subjects = X_korea_2006_2010_01.drop(['Barthel_diff' ], axis=1)\n",
    "X_korea_2006_2010_common_subjects.drop(X_korea_2006_2010_common_subjects[X_korea_2006_2010_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_korea_2006_2010_common_subjects = X_korea_2006_2010_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_korea_2006_2010_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_2010_ids = X_korea_2006_2010_common_subjects['CASEID']\n",
    "print(X_korea_2006_2010_common_subjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2012_common_subjects = X_korea_2006_2012_01.drop(['Barthel_diff' ], axis=1)\n",
    "X_korea_2006_2012_common_subjects.drop(X_korea_2006_2012_common_subjects[X_korea_2006_2012_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_korea_2006_2012_common_subjects = X_korea_2006_2012_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_korea_2006_2012_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_2012_ids = X_korea_2006_2012_common_subjects['CASEID']\n",
    "print(X_korea_2006_2012_common_subjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2014_common_subjects = X_korea_2006_2014_01.drop(['Barthel_diff' ], axis=1)\n",
    "X_korea_2006_2014_common_subjects.drop(X_korea_2006_2014_common_subjects[X_korea_2006_2014_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_korea_2006_2014_common_subjects = X_korea_2006_2014_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_korea_2006_2014_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_2014_ids = X_korea_2006_2014_common_subjects['CASEID']\n",
    "print(X_korea_2006_2014_common_subjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2016_common_subjects = X_korea_2006_2016_01.drop(['Barthel_diff' ], axis=1)\n",
    "X_korea_2006_2016_common_subjects.drop(X_korea_2006_2016_common_subjects[X_korea_2006_2016_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_korea_2006_2016_common_subjects = X_korea_2006_2016_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_korea_2006_2016_common_subjects.dropna(inplace=True)\n",
    "\n",
    "X_korea_2006_2016_ids = X_korea_2006_2016_common_subjects['CASEID']\n",
    "print(X_korea_2006_2016_common_subjects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = X_korea_2006_ids.copy()\n",
    "#merge = pd.merge(merge, X_korea_2006_2008_ids, on='CASEID', how='inner')\n",
    "#merge = pd.merge(merge, X_korea_2006_2010_ids, on='CASEID', how='inner')\n",
    "#merge = pd.merge(merge, X_korea_2006_2012_ids, on='CASEID', how='inner')\n",
    "merge = pd.merge(merge, X_korea_2006_2014_ids, on='CASEID', how='inner')\n",
    "merge = pd.merge(merge, X_korea_2006_2016_ids, on='CASEID', how='inner')\n",
    "\n",
    "merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_common_ids = pd.merge(X_korea_2006_common_subjects, merge, on='CASEID', how='inner')\n",
    "X_korea_2006_common_ids = X_korea_2006_common_ids.drop('CASEID', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2014_common_ids = pd.merge(X_korea_2006_2014_common_subjects, merge, on='CASEID', how='inner')\n",
    "X_korea_2006_2014_common_ids = X_korea_2006_2014_common_ids.drop('CASEID', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_korea_2006_2016_common_ids = pd.merge(X_korea_2006_2016_common_subjects, merge, on='CASEID', how='inner')\n",
    "X_korea_2006_2016_common_ids = X_korea_2006_2016_common_ids.drop('CASEID', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2006:', X_korea_2006_common_ids.shape[0], '\\t2014:', X_korea_2006_2014_common_ids.shape[0], '\\t2016:', X_korea_2006_2016_common_ids.shape[0])\n",
    "print('2006:', X_korea_2006_01.shape[0] - X_korea_2006_common_ids.shape[0], \n",
    "      '\\t2014:', X_korea_2006_2014_01.shape[0] - X_korea_2006_2014_common_ids.shape[0], \n",
    "      '\\t2016:', X_korea_2006_2016_01.shape[0] - X_korea_2006_2016_common_ids.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_01_common_subjects = X_costarica_01.drop(['Barthel_diff' ], axis=1)\n",
    "X_costarica_01_common_subjects.drop(X_costarica_01_common_subjects[X_costarica_01_common_subjects['MMSE_diff'] <0].index, inplace=True)\n",
    "X_costarica_01_common_subjects = X_costarica_01_common_subjects.drop(['MMSE_diff' ], axis=1)\n",
    "X_costarica_01_common_subjects.dropna(inplace=True)\n",
    "\n",
    "\n",
    "print(X_costarica_01_common_subjects.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_w1 = X_costarica_01_common_subjects.drop(['MMSE_w2' ], axis=1)\n",
    "X_costarica_w2 = X_costarica_01_common_subjects.drop(['MMSE' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w1:', X_costarica_w1.shape[0], '\\w2:', X_costarica_w2.shape[0])\n",
    "print('w1:', X_costarica_01.shape[0] - X_costarica_w1.shape[0], \n",
    "      '\\tw2:', X_costarica_01_common_subjects.shape[0] - X_costarica_w2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors, variables names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = pd.read_csv('../Data/cross/var_name_color_code_new.csv', encoding='latin-1', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = []\n",
    "\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    if(X_costarica_w1.columns[i] == 'FM_EstiloVida_Caida12Mes_c11_med'):\n",
    "        label = 'FM_EstiloVida_Caida12Mes_c11'\n",
    "    else:\n",
    "        label = X_costarica_w1.columns[i]\n",
    "    \n",
    "    index_ = list(X_cat.oldname).index(label)\n",
    "    new_name.append(list(X_cat.newname)[index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    print('', X_costarica_w1.columns[i], '\\n', X_costarica_w2.columns[i], '\\n', X_korea_2006_common_ids.columns[i], \n",
    "          '\\n', X_korea_2006_2014_common_ids.columns[i], '\\n', \n",
    "          X_korea_2006_2016_common_ids.columns[i], '\\n',new_name[i])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_costarica_w1.columns =  new_name\n",
    "X_costarica_w2.columns =  new_name\n",
    "X_korea_2006_common_ids.columns =  new_name\n",
    "#X_korea_2006_2012_common_ids.columns =  new_name\n",
    "X_korea_2006_2014_common_ids.columns =  new_name\n",
    "X_korea_2006_2016_common_ids.columns =  new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Live Alone', 'Falls']\n",
    "X_costarica_w1.drop(to_drop, axis=1, inplace=True)\n",
    "X_costarica_w2.drop(to_drop, axis=1, inplace=True)\n",
    "X_korea_2006_common_ids.drop(to_drop, axis=1, inplace=True)\n",
    "X_korea_2006_2014_common_ids.drop(to_drop, axis=1, inplace=True)\n",
    "X_korea_2006_2016_common_ids.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for i in range(len(X_costarica_w1.columns)):\n",
    "    print('', X_costarica_w1.columns[i], '\\n', X_costarica_w2.columns[i], '\\n', X_korea_2006_common_ids.columns[i],\n",
    "          '\\n', X_korea_2006_2014_common_ids.columns[i], '\\n', \n",
    "          X_korea_2006_2016_common_ids.columns[i], '\\n',new_name[i])\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_korea_2006 = X_korea_2006_common_ids['MMSE']\n",
    "X_korea_2006 = X_korea_2006_common_ids.drop('MMSE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_korea_2006, y_korea_2006, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_korea_2006 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_korea_2006.fit(X_korea_2006, y_korea_2006)\n",
    "\n",
    "parameter_dict['korea_2006'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_korea_2006 = summary(modelRidge_korea_2006, X_korea_2006, y_korea_2006, X_korea_2006.columns, 'Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_korea_2006_2014 = X_korea_2006_2014_common_ids['MMSE']\n",
    "X_korea_2006_2014 = X_korea_2006_2014_common_ids.drop('MMSE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_korea_2006_2014, y_korea_2006_2014, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_korea_2006_2014 = Ridge(alpha=0.001, solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_korea_2006_2014.fit(X_korea_2006_2014, y_korea_2006_2014)\n",
    "\n",
    "parameter_dict['korea_2006_2014'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_korea_2006_2014 = summary(modelRidge_korea_2006_2014, X_korea_2006_2014, y_korea_2006_2014, X_korea_2006_2014.columns, 'Ridge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_korea_2006_2016 = X_korea_2006_2016_common_ids['MMSE']\n",
    "X_korea_2006_2016 = X_korea_2006_2016_common_ids.drop('MMSE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_korea_2006_2016, y_korea_2006_2016, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_korea_2006_2016 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_korea_2006_2016.fit(X_korea_2006_2016, y_korea_2006_2016)\n",
    "\n",
    "parameter_dict['korea_2006_2016'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_korea_2006_2016 = summary(modelRidge_korea_2006_2016, X_korea_2006_2016, y_korea_2006_2016, X_korea_2006_2016.columns, 'Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_costarica_1 = X_costarica_w1['MMSE']\n",
    "X_costarica_1 = X_costarica_w1.drop(['MMSE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_costarica_1, y_costarica_1, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"val. score: %s\" % opt_Ridge.best_score_)\n",
    "print(\"test score: %s\" % opt_Ridge.score(X_test, y_test))\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_costarica_1 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_costarica_1.fit(X_costarica_1, y_costarica_1)\n",
    "\n",
    "parameter_dict['costarica_wave_1'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_costarica_1 = summary(modelRidge_costarica_1, X_costarica_1, y_costarica_1, X_costarica_1.columns, 'Ridge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_costarica_2 = X_costarica_w2['MMSE']\n",
    "X_costarica_2 = X_costarica_w2.drop(['MMSE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_costarica_2, y_costarica_2, train_size=0.75, test_size=.25, random_state=0)\n",
    "\n",
    "opt_Ridge = BayesSearchCV(\n",
    "    Ridge(),\n",
    "    {\n",
    "        'alpha': (1.0, 0.1, 0.01, 0.001),\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "        'max_iter': (1000, 10000, 100000, 1000000),\n",
    "        #'tol:' : (1e-6, 1e-3, 1e+1),\n",
    "        #'n_estimators': (100, 1000),\n",
    "\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Ridge')\n",
    "print(\"val. score: %s\" % opt_Ridge.best_score_)\n",
    "print(\"test score: %s\" % opt_Ridge.score(X_test, y_test))\n",
    "print(\"best parameters: %s\" % str(opt_Ridge.best_params_))\n",
    "print('---------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelRidge_costarica_2 = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                         max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "modelRidge_costarica_2.fit(X_costarica_2, y_costarica_2)\n",
    "\n",
    "parameter_dict['costarica_wave_2'] = opt_Ridge.best_params_\n",
    "\n",
    "modelRidge_sum_costarica_2 = summary(modelRidge_costarica_2, X_costarica_2, y_costarica_2, X_costarica_2.columns, 'Ridge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistically significant variables in both Costa Rica wave 2 and Korea 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = get_order_var(modelRidge_sum_costarica_2)\n",
    "df = modelRidge_sum_costarica_2\n",
    "\n",
    "feature_sign = []\n",
    "\n",
    "for i in col:\n",
    "    if df[0]['p value'][i] < 0.05:\n",
    "        feature_sign.append(i)\n",
    "\n",
    "\n",
    "df = modelRidge_sum_korea_2006_2016\n",
    "\n",
    "for i in col:\n",
    "    if df[0]['p value'][i] < 0.05 and i not in feature_sign:\n",
    "        feature_sign.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Costa Rica\n",
    "\n",
    "df_costarica_w2 = modelRidge_sum_costarica_2[0].copy()\n",
    "\n",
    "df_costarica_w2.index.name = 'Features'\n",
    "df_costarica_w2 = df_costarica_w2.iloc[1:-3, 0:-1]\n",
    "\n",
    "df_costarica_w2 = df_costarica_w2.loc[feature_sign, :]\n",
    "\n",
    "for i in range(3):\n",
    "    df_costarica_w2[df_costarica_w2.columns[i]] = np.abs(pd.to_numeric(df_costarica_w2[df_costarica_w2.columns[i]],errors = 'coerce'))\n",
    "\n",
    "df = df_costarica_w2.reset_index()\n",
    "data_costarica_w2 = df_costarica_w2.sort_values('Estimate', ascending=False)\n",
    "\n",
    "print('Ranking: Stat. Significant vars -  Costa Rica wave 2')\n",
    "print(pd.DataFrame(data_costarica_w2['Estimate']))\n",
    "\n",
    "## Korea\n",
    "\n",
    "df_korea_2016 = modelRidge_sum_korea_2006_2016[0].copy()\n",
    "\n",
    "df_korea_2016.index.name = 'Features'\n",
    "df_korea_2016 = df_korea_2016.iloc[1:-3, 0:-1]\n",
    "\n",
    "df_korea_2016 = df_korea_2016.loc[feature_sign, :]\n",
    "\n",
    "for i in range(3):\n",
    "    df_korea_2016[df_korea_2016.columns[i]] = np.abs(pd.to_numeric(df_korea_2016[df_korea_2016.columns[i]],errors = 'coerce'))\n",
    "\n",
    "df = df_korea_2016.reset_index()\n",
    "data_korea_2016 = df_korea_2016.sort_values('Estimate', ascending=False)\n",
    "\n",
    "print('\\n\\nRanking: Stat. Significant vars -   Korea 2016')\n",
    "print(pd.DataFrame(data_korea_2016['Estimate']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test for Coefficients between Costa Rica wave 2 and Korea 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_dict = {}\n",
    "for i in feature_sign:\n",
    "\n",
    "    beta_dict['CR_' + i] = []\n",
    "    beta_dict['KO_' + i] = []\n",
    "\n",
    "\n",
    "y_database = X_costarica_w2['MMSE']\n",
    "X_database = X_costarica_w2.drop('MMSE', axis=1)\n",
    "X_database = X_database[feature_sign]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,100,10):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "    opt_Ridge = BayesSearchCV(\n",
    "            Ridge(),\n",
    "            {\n",
    "                'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                'max_iter': (1000, 10000, 100000, 1000000),\n",
    "            },\n",
    "            n_iter=10,\n",
    "            random_state=i, \n",
    "            scoring='r2',\n",
    "            cv=3\n",
    "        )\n",
    "\n",
    "    opt_Ridge.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    model = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                             max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "    \n",
    "    model.fit(X_test, y_test)\n",
    "    \n",
    "    coef_df = pd.DataFrame(\n",
    "        index=['_intercept'] + list(X_test.columns),\n",
    "        columns=['Estimate']\n",
    "        )\n",
    "\n",
    "    coef_df['Estimate'] = np.concatenate((np.round(np.array([model.intercept_]), 12), np.round((model.coef_), 12)))\n",
    "\n",
    "    for j in feature_sign:\n",
    "        beta_dict['CR_' + j].append(np.abs(coef_df['Estimate'][j])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_database = X_korea_2006_2016_common_ids['MMSE']\n",
    "X_database = X_korea_2006_2016_common_ids.drop('MMSE', axis=1)\n",
    "X_database = X_database[feature_sign]\n",
    "\n",
    "for i in range(0,100,10):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "    opt_Ridge = BayesSearchCV(\n",
    "            Ridge(),\n",
    "            {\n",
    "                'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                'max_iter': (1000, 10000, 100000, 1000000),\n",
    "            },\n",
    "            n_iter=10,\n",
    "            random_state=i, \n",
    "            scoring='r2',\n",
    "            cv=3\n",
    "        )\n",
    "\n",
    "    opt_Ridge.fit(X_train, y_train)\n",
    "    \n",
    "    model = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                             max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "    \n",
    "    model.fit(X_test, y_test)\n",
    "    \n",
    "    coef_df = pd.DataFrame(\n",
    "        index=['_intercept'] + list(X_test.columns),\n",
    "        columns=['Estimate']\n",
    "        )\n",
    "\n",
    "    coef_df['Estimate'] = np.concatenate((np.round(np.array([model.intercept_]), 12), np.round((model.coef_), 12)))\n",
    "\n",
    "    for j in feature_sign:\n",
    "        beta_dict['KO_'+ j].append(np.abs(coef_df['Estimate'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = pd.DataFrame(beta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api\n",
    "\n",
    "result_scipy_df = pd.DataFrame(\n",
    "        index=['p value'] ,\n",
    "        columns=feature_sign\n",
    "        )\n",
    "\n",
    "result_stats_df = pd.DataFrame(\n",
    "        index=['p value'] ,\n",
    "        columns=feature_sign\n",
    "        )\n",
    "\n",
    "for i in feature_sign:\n",
    "    result_scipy_df[i] = stats.ttest_ind(beta_dict['CR_'+ i], beta_dict['KO_' + i], equal_var=False)[1]\n",
    "    result_stats_df[i] = statsmodels.stats.weightstats.ttest_ind(beta_dict['CR_'+ i], beta_dict['KO_' + i], alternative=\"two-sided\",usevar=\"unequal\")[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df_stat = beta_df.describe().round(3).iloc[1:3,:]\n",
    "beta_df_stat = beta_df_stat.append(beta_df_stat.iloc[1,:]*1.95,ignore_index=True)\n",
    "beta_df_stat = beta_df_stat.append(beta_df_stat.iloc[1,:]*2.58,ignore_index=True)\n",
    "\n",
    "beta_df_stat = beta_df_stat.rename(index={0:'mean', 1:'std', 2:'95', 3:'99'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sign_order = ['Education', 'Sex', 'Age', 'Physical activity', 'Mental Problems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_result_df = pd.DataFrame(\n",
    "        index=feature_sign_order ,\n",
    "        columns=['CR_mean', 'CR_std','KO_mean','KO_std', 'ttest']\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in feature_sign_order:\n",
    "    general_result_df.iloc[count, 0] = np.round(beta_df_stat['CR_' + i][0],2)\n",
    "    general_result_df.iloc[count, 1] = np.round(beta_df_stat['CR_' + i][1],2)\n",
    "    \n",
    "    general_result_df.iloc[count, 2] = np.round(beta_df_stat['KO_' + i][0],2)\n",
    "    general_result_df.iloc[count, 3] = np.round(beta_df_stat['KO_' + i][1],2)\n",
    "    \n",
    "    general_result_df.iloc[count, 4] = np.round(result_stats_df[i][0],2)\n",
    "    \n",
    "    count +=1\n",
    "    \n",
    "general_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_values_ttest(modelRidge_sum_model1, modelRidge_sum_model2, X_1, X_2, only_significance = False):\n",
    "    col = get_order_var(modelRidge_sum_model1)\n",
    "    df = modelRidge_sum_model1\n",
    "\n",
    "    feature_sign = []\n",
    "\n",
    "    for i in col:\n",
    "        if df[0]['p value'][i] < 0.05:\n",
    "            feature_sign.append(i)\n",
    "\n",
    "\n",
    "    df = modelRidge_sum_model2\n",
    "\n",
    "    for i in col:\n",
    "        if df[0]['p value'][i] < 0.05 and i not in feature_sign:\n",
    "            feature_sign.append(i)\n",
    "\n",
    "\n",
    "    beta_dict = {}\n",
    "    for i in feature_sign:\n",
    "\n",
    "        beta_dict['CR_' + i] = []\n",
    "        beta_dict['KO_' + i] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_database = X_1['MMSE']\n",
    "    X_database = X_1.drop('MMSE', axis=1)\n",
    "    \n",
    "    if(only_significance):\n",
    "        X_database = X_database[feature_sign]\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0,100,10):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "        opt_Ridge = BayesSearchCV(\n",
    "                Ridge(),\n",
    "                {\n",
    "                    'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                    'max_iter': (1000, 10000, 100000, 1000000),\n",
    "                },\n",
    "                n_iter=10,\n",
    "                random_state=i, \n",
    "                scoring='r2',\n",
    "                cv=3\n",
    "            )\n",
    "\n",
    "        opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        model = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                                 max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "\n",
    "        model.fit(X_test, y_test)\n",
    "\n",
    "        coef_df = pd.DataFrame(\n",
    "            index=['_intercept'] + list(X_test.columns),\n",
    "            columns=['Estimate']\n",
    "            )\n",
    "\n",
    "        coef_df['Estimate'] = np.concatenate((np.round(np.array([model.intercept_]), 12), np.round((model.coef_), 12)))\n",
    "\n",
    "        for j in feature_sign:\n",
    "            beta_dict['CR_' + j].append(np.abs(coef_df['Estimate'][j])) \n",
    "\n",
    "\n",
    "    y_database = X_2['MMSE']\n",
    "    X_database = X_2.drop('MMSE', axis=1)\n",
    "    if(only_significance):\n",
    "        X_database = X_database[feature_sign]\n",
    "\n",
    "    for i in range(0,100,10):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_database, y_database, train_size=0.75, test_size=.25, random_state=i)\n",
    "\n",
    "        opt_Ridge = BayesSearchCV(\n",
    "                Ridge(),\n",
    "                {\n",
    "                    'alpha': ( 0.0001, 0.01, 0.001),\n",
    "                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                    'max_iter': (1000, 10000, 100000, 1000000),\n",
    "                },\n",
    "                n_iter=10,\n",
    "                random_state=i, \n",
    "                scoring='r2',\n",
    "                cv=3\n",
    "            )\n",
    "\n",
    "        opt_Ridge.fit(X_train, y_train)\n",
    "\n",
    "        model = Ridge(alpha=opt_Ridge.best_params_['alpha'], solver = opt_Ridge.best_params_['solver'], \n",
    "                                 max_iter=opt_Ridge.best_params_['max_iter'])\n",
    "\n",
    "        model.fit(X_test, y_test)\n",
    "\n",
    "        coef_df = pd.DataFrame(\n",
    "            index=['_intercept'] + list(X_test.columns),\n",
    "            columns=['Estimate']\n",
    "            )\n",
    "\n",
    "        coef_df['Estimate'] = np.concatenate((np.round(np.array([model.intercept_]), 12), np.round((model.coef_), 12)))\n",
    "\n",
    "        for j in feature_sign:\n",
    "            beta_dict['KO_'+ j].append(np.abs(coef_df['Estimate'][j]))\n",
    "\n",
    "    beta_df = pd.DataFrame(beta_dict)\n",
    "\n",
    "\n",
    "    from scipy import stats\n",
    "    import statsmodels.api\n",
    "\n",
    "    result_scipy_df = pd.DataFrame(\n",
    "            index=['p value'] ,\n",
    "            columns=feature_sign\n",
    "            )\n",
    "\n",
    "    result_stats_df = pd.DataFrame(\n",
    "            index=['p value'] ,\n",
    "            columns=feature_sign\n",
    "            )\n",
    "\n",
    "    for i in feature_sign:\n",
    "        result_scipy_df[i] = stats.ttest_ind(beta_dict['CR_'+ i], beta_dict['KO_' + i], equal_var=False)[1]\n",
    "        result_stats_df[i] = statsmodels.stats.weightstats.ttest_ind(beta_dict['CR_'+ i], beta_dict['KO_' + i], alternative=\"two-sided\",usevar=\"unequal\")[1]\n",
    "\n",
    "\n",
    "    beta_df_stat = beta_df.describe().round(3).iloc[1:3,:]\n",
    "    beta_df_stat = beta_df_stat.append(beta_df_stat.iloc[1,:]*1.95,ignore_index=True)\n",
    "    beta_df_stat = beta_df_stat.append(beta_df_stat.iloc[1,:]*2.58,ignore_index=True)\n",
    "\n",
    "    beta_df_stat = beta_df_stat.rename(index={0:'mean', 1:'std', 2:'95', 3:'99'})\n",
    "    beta_df_stat.round(3)\n",
    "\n",
    "\n",
    "    general_result_df = pd.DataFrame(\n",
    "            index=feature_sign,\n",
    "            columns=['CR_mean', 'CR_std','KO_mean','KO_std', 'ttest']\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for i in feature_sign_order:\n",
    "        general_result_df.iloc[count, 0] = np.round(beta_df_stat['CR_' + i][0],2)\n",
    "        general_result_df.iloc[count, 1] = np.round(beta_df_stat['CR_' + i][1],2)\n",
    "\n",
    "        general_result_df.iloc[count, 2] = np.round(beta_df_stat['KO_' + i][0],2)\n",
    "        general_result_df.iloc[count, 3] = np.round(beta_df_stat['KO_' + i][1],2)\n",
    "\n",
    "        general_result_df.iloc[count, 4] = np.round(result_stats_df[i][0],2)\n",
    "\n",
    "        count +=1\n",
    "\n",
    "    return general_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coef_t_test =beta_values_ttest(modelRidge_sum_costarica_2, modelRidge_sum_korea_2006_2016, X_costarica_w2, X_korea_2006_2016_common_ids, True)\n",
    "Coef_t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_stat(clf, X, y):\n",
    "    \"\"\"Calculate summary F-statistic for beta coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : sklearn.linear_model\n",
    "        A scikit-learn linear model classifier with a `predict()` method.\n",
    "    X : numpy.ndarray\n",
    "        Training data used to fit the classifier.\n",
    "    y : numpy.ndarray\n",
    "        Target training values, of shape = [n_samples].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F-statistic value.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    return (r_squared / p) / ((1 - r_squared) / (n - p - 1))\n",
    "\n",
    "\n",
    "def f_stat_pvalue(clf, X, y):\n",
    "    \"\"\"Calculate summary F-statistic p value for beta coefficients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : sklearn.linear_model\n",
    "        A scikit-learn linear model classifier with a `predict()` method.\n",
    "    X : numpy.ndarray\n",
    "        Training data used to fit the classifier.\n",
    "    y : numpy.ndarray\n",
    "        Target training values, of shape = [n_samples].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F-statistic p value.\n",
    "    \"\"\"\n",
    "    n = X.shape[0] # Esto se extrae par los grados de libertad del numeador y el denomindor (no. predictores, no. sujetos - no. predictores-1)\n",
    "    p = X.shape[1]\n",
    "    r_squared = metrics.r2_score(y, clf.predict(X))\n",
    "    \n",
    "    return np.round(scipy.stats.f.sf(f_stat(clf, X, y), n, (n - p - 1)), 15)\n",
    "\n",
    "def compute_f_statistics(clf, X, y):\n",
    "    return [f_stat(clf, X, y), f_stat_pvalue(clf, X, y)]\n",
    "\n",
    "F_statistics = {}\n",
    "\n",
    "F_statistics['Korea 2006'] = [compute_f_statistics(modelRidge_korea_2006, X_korea_2006, y_korea_2006)]\n",
    "F_statistics['Korea 2006-2014'] = [compute_f_statistics(modelRidge_korea_2006_2014, X_korea_2006_2014, y_korea_2006_2014)]\n",
    "F_statistics['Korea 2006-2016'] = [compute_f_statistics(modelRidge_korea_2006_2016, X_korea_2006_2016, y_korea_2006_2016)]\n",
    "\n",
    "\n",
    "F_statistics['Costa Rica wave 1'] = [compute_f_statistics(modelRidge_costarica_1, X_costarica_1, y_costarica_1)]\n",
    "F_statistics['Costa Rica wave 2'] = [compute_f_statistics(modelRidge_costarica_2, X_costarica_2, y_costarica_2)]\n",
    "\n",
    "\n",
    "F_statistics_pd = pd.DataFrame(F_statistics, index = ['F-statisitcs, F-pvalue']).T\n",
    "F_statistics_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumamry Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_korea_2006[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_statistics_pd.iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006 - 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_korea_2006_2014[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_statistics_pd.iloc[1:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea 2006 - 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_korea_2006_2016[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_statistics_pd.iloc[2:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_costarica_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_statistics_pd.iloc[3:4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costa Rica wave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRidge_sum_costarica_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_statistics_pd.iloc[4:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_pd = pd.DataFrame(parameter_dict).T\n",
    "parameter_pd#.to_excel('Results/longitudinal/MMSE_hyperparameters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_interval_dict = {}\n",
    "\n",
    "conf_interval_dict['korea_2006'] = [conf_interval(X_korea_2006_common_ids)]\n",
    "conf_interval_dict['korea_2006_2014'] = [conf_interval(X_korea_2006_2014_common_ids)]\n",
    "conf_interval_dict['korea_2006_2016'] = [conf_interval(X_korea_2006_2016_common_ids)]\n",
    "\n",
    "\n",
    "conf_interval_dict['costarica_wave_1'] = [conf_interval(X_costarica_w1)]\n",
    "conf_interval_dict['costarica_wave_w2'] = [conf_interval(X_costarica_w2)]\n",
    "\n",
    "\n",
    "conf_interval_pd = pd.DataFrame(conf_interval_dict, index = ['conf interval']).T\n",
    "conf_interval_pd#.to_excel('Results/longitudinal/MMSE_conf_interval.xlsx')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
